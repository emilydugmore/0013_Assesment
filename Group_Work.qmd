---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
execute:
  echo: false
  freeze: true
  error: true
  warning: false
  message: false
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    mainfont: "Source Serif 4"
    sansfont: "Source Sans 3"
    monofont: "Source Code Pro"
    papersize: a4
    geometry:
      - top=25mm
      - left=25mm
      - right=25mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
    include-in-header:
      text: |
        \usepackage{graphicx}
        \usepackage{eso-pic}  
        \usepackage{transparent}
        \usepackage{xcolor}
        \definecolor{AgencyRed}{HTML}{8B0000}
        \definecolor{AgencyBlack}{HTML}{1A1A1A}

        \usepackage{fancyhdr}
        \pagestyle{fancy}
        \fancyhf{}
        \fancyhead[L]{\textbf{\thepage}}  % 
        \fancyhead[C]{\texttt{\textbf{\textcolor{AgencyRed}{TOP SECRET}}}} % 
        \fancyhead[R]{\texttt{Case File: London-Airbnb}} % 
        
        \fancyfoot[L]{} % 
        \fancyfoot[C]{} % 
        \fancyfoot[R]{\tiny \texttt{Generated by TNE$^2$+I Intelligence Unit}} % 
        
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

\begin{titlepage}
    % 使用 AddToShipoutPictureBG* 
    \AddToShipoutPictureBG*{%
        \AtPageLowerLeft{%
            \includegraphics[width=\paperwidth,height=\paperheight]{cover.png}%
        }%
    }
    \mbox{}
\end{titlepage}
\newpage



\newpage
\vspace*{1cm}

\begin{center}
\renewcommand{\arraystretch}{1.5} 
\setlength{\arrayrulewidth}{1pt}  

\begin{tabular}{|p{0.9\textwidth}|}
    \hline
    \textbf{\texttt{>> SECTION 1: INTEGRITY PLEDGE}} \\
    \hline
    \small
    \textit{We, "TNE$^2$ + I", pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. AI contributions have been clearly documented.} \\
    \\
    \textbf{TIMESTAMP:} \today \\
    \hline
    
    \textbf{\texttt{>> SECTION 2: TNE$^2$ + I (PERSONNEL)}} \\
    \hline
    \vspace{0.2cm}
    \begin{tabular}{p{0.6\textwidth} l}
        \texttt{Nadia Jimena Cabrera Salazar} & \textbf{[ID: 25198678]} \\
        \texttt{Emily Dugmore} & \textbf{[ID: 25060351]} \\
        \texttt{Julissa Tabata Paredes Condor} & \textbf{[ID: 25153376]} \\
        \texttt{Jiaying LI} & \textbf{[ID: 25211516]} \\
        \texttt{Emily Deeb} & \textbf{[ID: 25127081]} \\
    \end{tabular}
    \vspace{0.2cm}
    \\
    \hline

    \textbf{\texttt{>> SECTION 3: INTELLIGENCE REQUEST (FEEDBACK)}} \\
    \hline
    \vspace{0.1cm}
    \textbf{HQ is requested to provide critical analysis on the following sectors:}
    \begin{itemize}
    \item \textbf{Methodology Verification (Profit Loss Model)}: Our core analysis simulates the financial impact of the council tax proposal (Figure 3.3). We assumed that council tax is a valid proxy for operating costs. Is this simplification robust for a policy briefing, or are there standard models for this that we should have considered?

    \item \textbf{Visual Decryption (Landlord Concentration)}: To illustrate the dominance of professional landlords, we mapped the top 6 operators (Figure 2.3). Is this multi-plot layout effective in conveying market concentration, or would a single, aggregated heatmap be more impactful for a mayoral audience?

    \item \textbf{Narrative Logic (Policy Recommendation)}: Our central argument is that the proposed flat tax increase is a blunt instrument that disproportionately affects smaller operators in outer boroughs. Does our narrative, particularly the summaries for Figures 3.1-3.3, build a convincing case for this conclusion?
    \end{itemize}
    \vspace{0.2cm}
    \\ \hline
\end{tabular}
\end{center}

\vspace{1cm}
\begin{center}
    \textbf{\texttt{*** END OF DOSSIER HEADER ***}}
\end{center}
\newpage




{{< pagebreak >}}

```{python}
#| echo: false
#| output: false

import matplotlib.pyplot as plt
import seaborn as sns

# --- MI6 Theme Definition ---
mi6_red = '#8B0000'    #
mi6_black = '#3a3a3aff'  
mi6_grey = '#E0E0E0'   


plt.rcParams.update({
    'font.family': 'sans-serif',      
    'text.color': mi6_black,        
    'axes.labelcolor': mi6_black,       
    'xtick.color': mi6_black,           
    'ytick.color': mi6_black,
    'axes.edgecolor': mi6_black,        
    'axes.titlesize': 12,               
    'axes.titleweight': 'bold',         
    

    'axes.prop_cycle': plt.cycler(color=[mi6_red, mi6_black, '#555555']),
})
```

```{python}
#| output: false 
import os
from urllib.request import urlretrieve

# 1. Setup URLs
# Using the raw links from our GitHub repository
bib_url = "https://raw.githubusercontent.com/JIAYING-ICE/0013_Assesment/main/bio.bib"
csl_url = "https://raw.githubusercontent.com/JIAYING-ICE/0013_Assesment/main/harvard-cite-them-right.csl"
img_url = "https://raw.githubusercontent.com/JIAYING-ICE/0013_Assesment/main/Cover.png"
img_url = "https://raw.githubusercontent.com/JIAYING-ICE/0013_Assesment/main/Cover_2.png"

# 2. Download Function 
# Check if file exists, if not, download it.
def get_remote_file(file_name, url):
    if not os.path.exists(file_name):
        print(f"Downloading {file_name} from GitHub...")
        urlretrieve(url, file_name)
    else:
        print(f"{file_name} already exists.")

# 3. Execute 
get_remote_file("bio.bib", bib_url)
get_remote_file("harvard-cite-them-right.csl", csl_url)
get_remote_file("Cover.png", img_url)
get_remote_file("Cover_2.png", img_url)
```




{{< pagebreak >}}

# **INTRODUCTION** 
In the last few years, several cities across the world have experienced an explosive growth in Airbnb. What started as a shared economy platform for informal home-sharing has recently transformed into a major short-term rental market which fills the gap between traditional residential rental housing and hotel accommodation (@quattrone2016). While Airbnb advocates argue that the platform brings extra incomes to its users and new economic activities to cities, many raise concerns about its impact on housing markets through loss of housing supply, increased rent and gentrification (@mcgilluniversity).  

This report is motivated by claims that Airbnb is ‘out of control’ in London, and specifically addresses the recent proposal to force all professional landlords operating on the platform to register their properties and face higher Council Tax rates. Using comprehensive empirical analysis, we aim to answer five questions:  

1.  Is Airbnb ‘out of control’ in London?  
2.  How many professional landlords are there?  
3.  How many properties would be affected by the proposal?  
4.  What are the likely pros and cons of the proposal (for the Mayor, residents, and the city)?  
5.  Is it possible that Airbnb is contributing positively to social mobility or housing opportunities?  


#### **Data and methodologies**

Our investigation uses listing data collected by Inside Airbnb  – an independent, non-commercial website that scrapes publicly available information from the Airbnb platform (@insideairbnb). The IA dataset offers a more “transparent” view of Airbnb activity, and is commonly used in research. However, it is not without serious limitations which need consideration.  

Importantly, the dataset acts as a snapshot of listing information as it appeared on the day of scraping, making much of the data unreliable because hosts could modify listing information at any moment. In 2015, Airbnb stopped disclosing the difference between nights that are booked by guests and nights that are ‘blacked-out’ by hosts, making it impossible to precisely measure occupancy and revenue. We use the estimated occupancy and revenue generated from the IA occupancy model (@mcgilluniversity). The coordinates for listings in the IA dataset are shifted 150m in a random direction from the actual address of the listing. It is important to note that listings which fall close to borough boundaries may have been shifted into a different borough, slightly affecting the precision of the results.  

To determine the impact of Airbnb on the housing market, we use average monthly rents in the private rental market (July 2023 - June 204). Council Tax data from 2024 is used to investigate the potential impact of introducing higher Council Tax rates to professional landlords. Both datasets are aggregated at the borough level – a spatial unit which also acts as administrative boundaries – making our results relevant for Local Planning Authorities and actionable for local policies.  

{{< pagebreak >}}

::: {.callout-tip}
#### **Assumptions**
Other limitations that might need discussing (depending on how many words we have at the end):

  - | Although each Airbnb listing is specified on the public Airbnb website with exact latitude and longitude coordinates,
      these coordinates have been shifted from the real location by up to 150m in a random direction (in order to protect hosts’ privacy).
      This randomness means that maps which show the exact locations of listings (or rely on these locations for their analyses)
      are misleading inasmuch as they exaggerate the precision of the underlying spatial data.
  - NYC paper
:::



```{python}
#output: false 
# --- Part 01 Data Loading ---
# The Work Mythology in Part 01 Data Loading and Initial Cleaning:
# Steps:
# 1) Import Packages
# 2) Ensuring Folder Structure, a main file named data, and subfolders: Raw and Clean.
# 3) Download files from the Web into a raw file in the home repository
# 4) Additional Data loading, such as converting and unzipping a  file
# 5) Identifying which columns to keep from the raw file structure
# 6) Build CLEAN versions in data/clean (from RAW)
# 7) Load from data/clean with selected columns
# 8) Clean column names function that does the initial cleaning on the data before in-depth analysis
```

```{python}
#| output: false 
# Step 01 - Import packages needed for analysis
import os
import urllib.request
import zipfile
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
```

```{python}
#| output: false 
# Step 02 - Ensuring Folder Structure: Create a main file named data and subfolders named raw and clean.
def ensure_data_folders():
    folders = ["data/raw", "data/clean"]
    
    for folder in folders:
        if not os.path.isdir(folder):
            os.makedirs(folder)
            print(f"Created folder: {folder}")
        else:
            print(f"Folder already exists: {folder}")

    print("--" * 30)

ensure_data_folders()
```

```{python}
#| output: false 
# Step 03 - Download files from the Web into a raw file on the home repository
# Please Note: The gitignore file includes the whole of the data file so that you won't see this file in the main repo

# function that downloads  files into raw only if they don't exist
def download_if_missing(url, local_name):
    dest = os.path.join("data", "raw", local_name)
    
    print(f"Let me check: {local_name}")

    if os.path.isfile(dest):
        print("File already exists — skipping download.")
    else:
        print(f"Downloading from:\n    {url}")
        urllib.request.urlretrieve(url, dest)
        print("Download complete — saved.")

    print("--" * 20)
    return dest 

# attribute url path to a variable Source: Prof. Jon Reades from the CASA department at UCL
base = "https://orca.casa.ucl.ac.uk/~jreades/data/"

# files we wish to download  from that path 
orca_files = [
    
    "20250615-London-listings.csv.gz",
    # ^ Airbnb listings from calendar "screenshot" 15/June/2025 (comma-separated values)
    "MSOA-2011.gpkg" 
    # ^ Middle-Layer Super Output Area for London (Spatial file)
]

# Extra files we saw necessary to add for further analysis (Outside the scope offered by Prof. Reades)
extra_files = {
    
    "housing_msoa.csv":
        "https://data.london.gov.uk/download/2z0yn/20264159-36cb-4aa2-8371-ae884ae83e88/msoa-data.csv",
        # ^ Housing data by MSOA boundaries to evaluate how many households are in a jurisdictional boundary (comma-separated values)    
        
    "londonrentalstatsaccessibleq22024.xlsx":
        "https://www.ons.gov.uk/file?uri=/economy/inflationandpriceindices/adhocs/2224privaterentalmarketinlondonjuly2023tojune2024/londonrentalstatsaccessibleq22024.xlsx",
       # ^ Average monthly Rent according to Borough (Excel must transform to comma-separated values)
   
    "london_boroughs.zip":
        "https://data.london.gov.uk/download/20od9/08d31995-dd27-423c-a987-57fe8e952990/London-wards-2018.zip",
        # ^ Borough Boundary of London (Spatial file)
    
    "council_tax_borough.xlsx":
        "https://data.london.gov.uk/download/expnl/59cc7c37-da8f-4158-bc47-491c3d167b05/council-tax-bands-borough.xlsx"
        # ^ Council Tax per Borough Band D (Excel must transform to comma-separated values)
}

local_paths = [] # Open List for later for-loop

# Orca files download to raw file
print("!STARTING ORCA DOWNLOADS!")
for fname in orca_files:
    url = base + fname
    path = download_if_missing(url, fname) 
    local_paths.append(path)

# Extra files download to raw file
print("!STARTING DATASTORE DOWNLOADS!")
for local_name, url in extra_files.items():
    path = download_if_missing(url, local_name)
    local_paths.append(path)
```

```{python}
#| output: false 
# Step 04 -  Additional Data loading, such as converting Excel to CSV and unzipping a file
#            in the condition that the files haven't done this

# Convert the Rent file from Excel to CSV
print("!CONVERTING RENT EXCEL (Number 2 sheet) TO CSV!")
    
excel_path = "data/raw/londonrentalstatsaccessibleq22024.xlsx"
csv_path   = "data/raw/londonrentalstatsaccessibleq22024.csv"

if os.path.isfile(csv_path):
    print(f"CSV already exists, skipping conversion: {csv_path}")
else:
    if not os.path.isfile(excel_path):
        print(f"Excel file not found: {excel_path}")
    else:
        rents_xlsx = pd.read_excel(
            excel_path,
            sheet_name="2",
            skiprows = 2
        )
        rents_xlsx.to_csv(csv_path, index=False)
        print(f"Converted Excel to CSV: {csv_path}")

print("--" * 20)


# Convert Council Tax file from Excel to CSV
print("!CONVERTING COUNCIL TAX EXCEL (2024-2025) TO CSV!")

excel_path = "data/raw/council_tax_borough.xlsx"
csv_path   = "data/raw/council_tax_borough.csv"

if os.path.isfile(csv_path):
    print(f"CSV already exists, skipping conversion: {csv_path}")
else:
    if not os.path.isfile(excel_path):
        print(f"Excel file not found: {excel_path}")
    else:
        tax_xlsx = pd.read_excel(
            excel_path,
            sheet_name="2024-25"
        )
        tax_xlsx.to_csv(csv_path, index=False)
        print(f"Converted Excel to CSV: {csv_path}")

print("--" * 20)

print("!UNZIPPING LONDON BOROUGHS SHAPEFILE!")

# unzipping Shape File of London Borough
zip_path = "data/raw/london_boroughs.zip"
if os.path.isfile(zip_path):
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall("data/raw/")
    print("Unzipped london_boroughs.zip")
else:
    print("Zip file not found: data/raw/london_boroughs.zip")

print("--" * 20)
```

```{python}
#| output: false 
# Step 05 - Identifying which columns to keep from raw file structure

# Columns needed for listings 
cols_listings = [
    'id', 'name', 'host_id', 'host_is_superhost', 
    'host_total_listings_count',
    'neighbourhood_cleansed', 'latitude', 'longitude', 
    'property_type', 'room_type', 'bedrooms', 'accommodates', 
    'price', 'minimum_nights', 'maximum_nights', 
    'minimum_minimum_nights','maximum_minimum_nights',
    'minimum_maximum_nights','maximum_maximum_nights',
    'minimum_nights_avg_ntm','maximum_nights_avg_ntm',
    'availability_365',
    'number_of_reviews', 
    'number_of_reviews_ltm',
    'first_review', 'last_review',
    'review_scores_rating', 
    'calculated_host_listings_count',
    'calculated_host_listings_count_entire_homes',
    'calculated_host_listings_count_private_rooms',
    'calculated_host_listings_count_shared_rooms',
    'reviews_per_month',
    'estimated_revenue_l365d',
    'estimated_occupancy_l365d'
]

# Housing (2011 only)*** The housing  is only from 2011 add as a problem for the data
cols_housing = ["Middle Super Output Area",
                 "MSOA Name",
                 "Households (2011);All Households;"
                 ]

# Council tax: band D comparison as it is the Band used as reference by the GLA !!! ADD REF
cols_tax = [
    "Code",
    "Local authority",
    "Band D"
]
```

```{python}
#| output: false 
# Step 06 -  Build CLEAN versions in data/clean (from RAW)
# Each File goes through the raw version and into clean which would allow us to manipulate the data without losing the raw

print("!CREATING CLEAN VERSIONS!")

# Save listings into a clean file
raw_listings_path = "data/raw/20250615-London-listings.csv.gz"
clean_listings_path = "data/clean/20250615-London-listings.csv.gz"

if not os.path.isfile(clean_listings_path):
    listings_raw = pd.read_csv(
        raw_listings_path,
        compression="gzip",
        encoding="latin1",
        low_memory=False
    )
    listings = listings_raw[cols_listings]
    listings.to_csv(
        clean_listings_path,
        index=False,
        compression="gzip"
    )
    print("Saved cleaned listings to data/clean")
else:
    print("Clean listings already exist, skipping.")

# save MSOA into clean file
raw_msoa_path = "data/raw/MSOA-2011.gpkg"
clean_msoa_path = "data/clean/MSOA-2011.gpkg"

if not os.path.isfile(clean_msoa_path):
    msoa = gpd.read_file(raw_msoa_path)
    msoa.to_file(clean_msoa_path, driver="GPKG")
    print("Saved MSOA to data/clean")
else:
    print("Clean MSOA already exists, skipping.")

# Save Rent into clean file (already converted to CSV in raw)
raw_rent_path = "data/raw/londonrentalstatsaccessibleq22024.csv"
clean_rent_path = "data/clean/londonrentalstatsaccessibleq22024.csv"

if not os.path.isfile(clean_rent_path):
    rent = pd.read_csv(raw_rent_path, encoding="latin1", low_memory=False)
    rent.to_csv(clean_rent_path, index=False)
    print("Saved cleaned rent to data/clean")
else:
    print("Clean rent already exists, skipping.")

# Save Housing into clean file
raw_housing_path = "data/raw/housing_msoa.csv"
clean_housing_path = "data/clean/housing_msoa.csv"

if not os.path.isfile(clean_housing_path):
    housing_raw = pd.read_csv(raw_housing_path, encoding="latin1", low_memory=False)
    housing = housing_raw[cols_housing]
    housing.to_csv(clean_housing_path, index=False)
    print("Saved cleaned housing to data/clean")
else:
    print("Clean housing already exists, skipping.")

# Save Borough and Wards into a clean file
borough_clean_dir = "data/clean/London-wards-2018_ESRI"
os.makedirs(borough_clean_dir, exist_ok=True)

# this path depends on how the zip unzips 
raw_borough_path = "data/raw/London-wards-2018_ESRI/London_Ward.shp"

clean_borough_path = os.path.join(borough_clean_dir, "London_Ward.shp")

if not os.path.isfile(clean_borough_path):
    borough = gpd.read_file(raw_borough_path)
    borough.to_file(clean_borough_path)
    print("Saved borough shapefile to data/clean")
else:
    print("Clean borough shapefile already exists, skipping.")

# Save Council Tax file into clean file
raw_tax_path = "data/raw/council_tax_borough.csv"
clean_tax_path = "data/clean/council_tax_borough.csv"

if not os.path.isfile(clean_tax_path):
    council_tax_raw = pd.read_csv(raw_tax_path, encoding="latin1", low_memory=False)
    council_tax = council_tax_raw[cols_tax]
    council_tax.to_csv(clean_tax_path, index=False)
    print("Saved cleaned council tax to data/clean")
else:
    print("Clean council tax already exists, skipping.")

print("**All cleaned files saved to data/clean/**")
```

```{python}
#| output: false 
# Step 07 -  Load from data/clean with selected columns and attirbute to a value 

print("!LOADING FILES (SELECTED COLUMNS ONLY)!")

# Load Listings
listings = pd.read_csv(
    "data/clean/20250615-London-listings.csv.gz",
    compression="gzip",
    usecols=cols_listings,
    encoding="latin1",
    low_memory=False
)
print(f"Step 01: listings loaded: {listings.shape[0]:,} rows, {listings.shape[1]} columns")

# Load MSOA
msoa = gpd.read_file("data/clean/MSOA-2011.gpkg")
print(f"Step 02: msoa loaded: {msoa.shape[0]:,} polygons, {msoa.shape[1]} columns")

# Rent
rent = pd.read_csv(
    "data/clean/londonrentalstatsaccessibleq22024.csv",
    encoding="latin1",
    low_memory=False
)
print(f"Step 03: rent loaded: {rent.shape[0]:,} rows, {rent.shape[1]} columns")

# Load Housing
housing = pd.read_csv(
    "data/clean/housing_msoa.csv",
    usecols=cols_housing,
    encoding="latin1",
    low_memory=False
)
print(f"Step 04: housing loaded: {housing.shape[0]:,} rows, {housing.shape[1]} columns")

# Load Borough boundary polygons
borough = gpd.read_file("data/clean/London-wards-2018_ESRI/London_Ward.shp")
print(f"Step 05: borough loaded: {borough.shape[0]:,} polygons, {borough.shape[1]} columns")

# Load Council Tax
council_tax = pd.read_csv(
    "data/clean/council_tax_borough.csv",
    usecols=cols_tax,
    encoding="latin1",
    low_memory=False
)
print(f"Step 06: council tax loaded: {council_tax.shape[0]:,} rows, {council_tax.shape[1]} columns")


# Print this when done
print("**All selected files successfully loaded.**")
```

```{python}
#| output: false 
# Step 08 - Clean column names function that does the initial cleaning on the data before in-depth analysis

# Create a function clean_names that runs through the data frame
def clean_names(df):
    df = df.copy()
    df.columns = (
        df.columns
        .str.strip()
        .str.lower()
        .str.replace(" ", "_")
        .str.replace(r"[^\w]", "", regex=True)
        .str.replace(",", "", regex=False)
    )
    return df

# turn data frame into a dictionary for fast cleaning
dfs = {
    "listings": listings,
    "msoa": msoa,
    "rent": rent,
    "housing": housing,
    "borough": borough,
    "council_tax": council_tax
}
for name, frame in dfs.items():
    dfs[name] = clean_names(frame)
    print(f"Cleaned column names for: {name}")

# Reverse the dictionary back to individual data frames 
listings     = dfs["listings"]
msoa         = dfs["msoa"]
rent         = dfs["rent"]
housing      = dfs["housing"]
borough      = dfs["borough"]
council_tax  = dfs["council_tax"]
```

```{python}
#| output: false 
# --- Part 02 Data Cleaning ---
# Removing null values, changing classes, merging values ...
# Steps:
# 1) Cleaning the LISTINGS Dataframe
# 2) Cleaning the BOROUGH Dataframe
```

```{python}
#| output: false 
# Step 01 - listings cleaning
probs = listings.isnull().sum(axis=1)
listings.isnull().sum(axis=1).sort_values(ascending=False).head(10)
```

```{python}
#| output: false 
# If a row has more than 5 null values, we remove and see as not fit data
print(f"listings contains {listings.shape[0]:,} rows.")
cutoff = 5
listings.drop(probs[probs > cutoff].index, inplace=True)
print(f"listings contains {listings.shape[0]:,} rows.")
```

```{python}
#| output: false 
# Check Where are the null values
listings.isnull().sum(axis=0).sort_values(ascending=False)
listings.info()
```

```{python}
#| output: false 
# Converting Values into readable values for data wrangling and analysis 
# boolean values
bools = ['host_is_superhost']
for b in bools:
    print(f"Converting {b} to boolean")
    listings[b] = listings[b].replace({'f':False, 't':True}).astype('bool')

# categories
cats = ['property_type','room_type']
for c in cats:
    print(f"Converting {c} to categort")
    listings[c] = listings[c].astype('category')

# dates
dates = ['first_review','last_review']
for d in dates:
    print(f"Converting {d} to date")
    listings[d] = pd.to_datetime(listings[d])

# strings
money = ['price']
for m in money:
    print(f"Converting {m} to float")
    listings[m] = (
        listings[m]
        .astype(str)            # convert to string so .str works
        .str.replace('$', '', regex=False)
        .str.replace(',', '')
        .replace('nan', float('nan'))  # optional: fix 'nan' strings
        .astype(float))

# integers
ints  = ['id','host_id','host_total_listings_count','bedrooms','accommodates', 'minimum_nights','maximum_nights','availability_365']
for i in ints:
    print(f"Converting {i} to integer")
    try:
        listings[i] = listings[i].astype('float').astype('int')
    except ValueError as e:
        print("  - !!!Converting to unsigned 16-bit integer!!!")
        listings[i] = listings[i].astype('float').astype(pd.UInt16Dtype())

listings.info()
```

```{python}
#| output: false 
# Step 02 -  Clean the Borough Shapefile

# Ensure CRS is correct to match Airbnb points later
borough = borough.to_crs(epsg=4326)

# Extract borough code from lagsscode
borough['borough_code'] = (
    borough['lagsscode']
    .astype(str)
    .str.strip())

# Clean borough name from shapefile
borough['borough_name'] = (
    borough['district']
    .astype(str)
    .str.lower()
    .str.replace("&", "and", regex=False)
    .str.replace("-", " ", regex=False)
    .str.strip())

# Keep only essential columns
borough_min = borough[['borough_code', 'borough_name', 'geometry']].copy()

# Dissolve all pieces per borough_code into a single geometry (outer boundary of borough)
borough_shp = borough_min.dissolve(by='borough_code')

# Bring borough_code back as a normal column
borough_shp = borough_shp.reset_index()

print("clean borough shapefile:", borough_shp.shape)
print("geom types:", borough_shp.geom_type.value_counts())
display(borough_shp.head())

# Quick visual check (DONT PRINT)
# ax = borough_shp.plot(edgecolor='black', facecolor='none', figsize=(6, 6))

# ---- Output Data -----
# borough_shp = Which is a cleaned, outer boundary (dissolved) shape file of the boroughs.
```

# **Question 01: Is Airbnb out of control in London?**  

Assessing whether Airbnb is “out of control” in London requires defining what this means. In this analysis, Airbnb becomes out of control when short-term rentals grow large enough to **(1) reduce the supply of long-term housing, (2) generate strong financial incentives to convert homes away from residential use, and (3) exceed the city’s regulatory capacity—particularly the 90-night annual limit for short-term lets.**

Based on the analysis, Airbnb does not appear uniformly “out of control” across London, but its impact is clearly concentrated, uneven, and significant in key areas of the city, particularly in Inner London and in larger family-sized homes.  

::: {.content-hidden}
** Work in progress**
:::


#### **Growth tendency in airbnb full-time short-term rentals**

The dataset does not include the exact date when a property was first listed on Airbnb. However, it does contain the date of the first review. Because hosts typically receive their first review shortly after a guest stays for the first time, **the year of the first review is generally a good proxy for the year the listing became active.**  

For this reason, we use the year of the first review as an estimate of the listing’s “introduction year.” Plotting a histogram of these years allows us to visualize how many new Airbnb listings entered the market each year and to observe the growth of the platform over time.  

The histogram shows a clear surge in the number of new listings from 2021 onwards, quickly recovering from the Covid-19 pandemic. The strong growth in recent years suggests that Airbnb supply has been accelerating rather than stabilizing.  

```{python}
#| output: false 
# -- The Work Methodology For evaluating If Airbnb is out of control in London:--
# Steps Overview:
# 1. Create a subset of Airbnb full-time listings
# 2. Spatial Join to assigns Listings to MSOA.
# 3. Review the Growth of Airbnb Listings over time
#    Output: **Figure 01**
# 4. Calculate the percentage of full-time Airbnb listings relative to 
#    total households at MSOA level and map the spatial distribution.
#    Output: **Figure 02**
# 5. Clean Rent data set (Could this be moved to data cleaning section? is a short code)
# 6. Classify Airbnb listings into bedroom categories so that 
#    they are directly comparable with the rent dataset.
# 7. Merge the rent data with the borough shapefile and the 
#    borough-level Airbnb revenue estimates.
# 8. Produce scatterplots comparing Airbnb monthly revenue and 
#    private rents by bedroom category.
#    Output: **Figure 03**
# 9. Map the rent gap (Airbnb revenue / private rent) for two-bedroom units 
#    across London boroughs.
#    Output: **Figure 04**
# 10. Zoom in on a high-pressure borough (e.g. Camden) to compare 
#     Airbnb revenue and private rent by unit size and illustrate 
#     local rent gaps in more detail.
#     Output: **Figure 05**
```

```{python}
#| output: false 
# Step 01: Create a subset of Airbnb full-time listings

# Filter listings where:

# (1) room_type = 'Entire home/apt' -> excludes private rooms/shared rooms
# (2) availability_365 > 90  -> proxy for full-time rental activity

listings_entirehome = listings[
    (listings["room_type"] == "Entire home/apt")
].copy()

listings_fulltime = listings_entirehome[
    (listings_entirehome["availability_365"] > 90) 
].copy()

#Print basic statistics to understand the subset

# What % of all listings are available > 90 days?
print(
    f"Out of {len(listings):,} listings in the Airbnb dataset, "
    f"{len(listings[listings['availability_365'] > 90]):,} listings "
    f"({(len(listings[listings['availability_365'] > 90]) / len(listings)) * 100:.1f}%) "
    f"are available more than 90 days in a year."
)

# What % of listings are whole-home units?
print(
    f"Out of {len(listings):,} listings in the Airbnb dataset, "
    f"{len(listings[listings['room_type'] == 'Entire home/apt']):,} listings "
    f"({(len(listings[listings['room_type'] == 'Entire home/apt']) / len(listings)) * 100:.1f}%) "
    f"are entire home/apartment units."
)

# What % meet both conditions (full-time & entire home)?
print(
    f"Out of {len(listings):,} listings in the Airbnb dataset, "
    f"{len(listings_fulltime):,} listings "
    f"({(len(listings_fulltime) / len(listings)) * 100:.1f}%) "
    f"are entire home/apartment unit, available for more than 90 days in a year."
)

# ---- Output Data -----
# listings_fulltime = Subset Availability >90 days and entire homes
```

```{python}
#| output: false 
# Step 02: Spatial Join to assign Listings to MSOA

# Convert listings full-time df into Geopandas

listings_fulltime = gpd.GeoDataFrame(
    listings_fulltime,
    geometry=gpd.points_from_xy(
        listings_fulltime.longitude,
        listings_fulltime.latitude
    ),
    crs="EPSG:4326"    
)

# Reproject to Britain National Grid 
listings_fulltime = listings_fulltime.to_crs(epsg=27700)
msoa = msoa.to_crs(epsg=27700)


# Spatial join: assign each Airbnb listing to the MSOA it falls within
# Creates a new dataframe where each listing gets assigned to an MSOA #### within
listings_sjoin = gpd.sjoin(
    listings_fulltime,
    msoa,
    how="left",
    predicate="within"
)

# ---- Output Data -----
# listings_sjoin = data frame of all full-time listings plus the MSOA name it falls within
```

```{python}
#| output: false 
listings_sjoin.head()
```

{{< pagebreak >}}

## **Figure 1.1** | Growth of Airbnb Listings

```{python}
#Step 03: Review the Growth of Airbnb Listings over time, for all of London and specifically for Inner London

# The focus on Inner London is because we hypothesize that most of the new Airbnb listings every year are concentrated in this area.
# Inner London boroughs are characterised by higher housing demand and lower housing supply (fewer housing completions per year)

# Data Limitations: 
#The data does not include the exact date when each listing was first published on the platform. However, it does record the
# date of the first review. Under the assumption that reviews are usually posted shortly after a guest's first stay,
# the year of the first review can be used as a proxy for the year the listing became active.

#Create a new column only with the year of the first review.
listings_sjoin["year_first_review"] = listings_sjoin["first_review"].dt.year.astype("Int64")


# Define the boroughs that belong to inner London 
inner_london = [
    "Camden",
    "Greenwich",
    "Hackney",
    "Hammersmith and Fulham",
    "Islington",
    "Kensington and Chelsea",
    "Lambeth",
    "Lewisham",
    "Southwark",
    "Tower Hamlets",
    "Wandsworth",
    "Westminster",
    "City of London"
]

# Create a df only for inner London
df_inner = listings_sjoin[
    listings_sjoin["lad11nm"].isin(inner_london)
].copy()

# Count the number of listings by year in all of London and Inner London
# Excludes 2025 as the data for the current year is incomplete
count_all = listings_sjoin[listings_sjoin["year_first_review"] != 2025]["year_first_review"].value_counts().sort_index()
count_inner = df_inner[df_inner["year_first_review"] != 2025]["year_first_review"].value_counts().sort_index()


# Plot: Airbnb listings added per year in All London vs Inner London
plt.figure(figsize=(11, 6)) #

# MI6 
mi6_red = '#8B0000'    # Inner London (The "Threat")
mi6_grey = '#606060'   # Outer London (The "Context")

count_all, count_inner = count_all.align(count_inner, fill_value=0)
count_outer = count_all - count_inner


# 1. Inner London 
plt.bar(
    count_inner.index.astype(str),
    count_inner.values,
    label="Inner London",
    color=mi6_red
)

# 2. Outer London 
plt.bar(
    count_outer.index.astype(str),
    count_outer.values,
    label="Outer London",
    color=mi6_grey,
    bottom=count_inner.values 
)


plt.xlabel("Year", fontsize=12, fontweight='bold')
plt.ylabel("Listings Added", fontsize=12, fontweight='bold')
plt.title(
    "Growth of Airbnb Listings: Inner vs Outer London", 
    fontsize=14, 
    fontweight='bold', 
    pad=20
)
plt.xticks(rotation=45, ha='right', fontsize=11)
plt.yticks(fontsize=11)
plt.legend(fontsize=12, loc='upper left', frameon=True, shadow=True)

ax = plt.gca()
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.grid(axis="y", alpha=0.3)
plt.tight_layout()
plt.show()

# ---- Output data ----
# df_inner: A df containing listings located only in the boroughs of Inner London
# ---- Output Figure ----
# Figure 01: Histogram of the Growth of Airbnb Listings over the years
#            (all London vs Inner London) to see the impact on Inner London, where the housing demand is much higher.
```

**Summary Figure 1.1**  

- According to the London Plan Annual Monitoring Report 2022–23 (@greaterlondonauthority), annual housing completions in London have shown relatively limited variation in recent years. Between 2018 and 2023, net housing completions averaged approximately 36,600 homes per year, with fluctuations of only a few thousand units across the period. This contrasts sharply with the rapid growth of Airbnb supply. In 2023, Airbnb added around 5000 new listings, and in 2024 this figure rose to more than 7,200, representing the equivalent of 13.8% to 20% of London’s yearly housing production. These proportions highlight that the rapid expansion of short-term rentals is effectively undermining the impact of London’s efforts to increase housing supply. Even as thousands of new homes are completed each year, a significant share appears to be absorbed into the Airbnb market rather than contributing to long-term housing availability.  

{{< pagebreak >}}

## **Figure 1.2** | Growth of Airbnb Listings

```{python}
#| output: false 
listings_sjoin.columns
```

```{python}
# Step 04: Percentage of full-time listings out of total households by MSOA

# Produce a normalised indicator of Airbnb intensity by calculating the proportion of full-time, entire-home listings relative to the total number of households in each MSOA. 
#This allows comparison across areas with different population sizes and reveals where Airbnb activity may exert stronger pressure on local housing supply.
# Count how many listings fall in each MSOA
listing_counts = listings_sjoin.groupby("msoa11nm").size()

# Merge the MSOA geometry from the GeoPackage with the household data so that each polygon includes the number of households in that area.
msoa_merged = msoa.merge(housing, left_on="msoa11nm", right_on="msoa_name")
msoa_merged = msoa_merged[["msoa_name", "households_2011all_households", "geometry"]]
msoa_merged = msoa_merged.to_crs(epsg=27700)
# Merge the listing counts back into the MSOA GeoDataFrame
msoa_merged = msoa_merged.merge(
    listing_counts.reset_index(name="listing_counts"),
    left_on="msoa_name",
    right_on="msoa11nm",
    how="left"
)

# Calculate the percentage of Airbnb listings relative to households per MSOA
msoa_merged["percentage"] = (msoa_merged["listing_counts"] / msoa_merged["households_2011all_households"]*100)


#Plot map 

fig, ax = plt.subplots(1, 1, figsize=(10, 8))

msoa_merged.plot(
    ax=ax,
    column="percentage",
    cmap="OrRd",
    scheme="NaturalBreaks",
    k=5,
    linewidth=0,
    legend=True,
    legend_kwds={
    "title": "",
    "fmt": "{:,.0f}"  # string, not method
}
)

# Make it look cleaner
ax.set_axis_off()
ax.set_title("Percentage of fulltime listings out of total households by MSOA", fontsize=12)

plt.show()

# ---- Output Figure ----
# Figure 02: Choropleth map visualising the percentage of full-time Airbnb 
#            entire-home listings relative to total households per MSOA.
```

**Summary Figure 1.2**   

- Conclusions from the map The map reveals a highly asymmetric spatial distribution of illegal Airbnb Activity in London. In contrast, most MSOAs in Outer London, show minimun levels of illegal Airbnb Activity. This suggests that Airbnb is not "out of control" everywhere, but is concentrated in central, high demand areas. **(This parragraph needs more work)**  
- Some of the darkest MSOAs—particularly in the City of London and a few central districts—reflect very high percentages of full-time Airbnb listings relative to local households.  
- This does not necessarily indicate unusually high Airbnb activity; rather, these areas have very small residential populations and low household counts. These values should therefore be interpreted with caution.  

```{python}
#| output: false 
# Step 05: Clean rent dataset (Could this be moved to data cleaning section? is a short code)

# Limitations: 
# We were only able to find current rental information at the borough level, not at the MSOA level.
# As a result, the analysis must be conducted using a larger spatial unit than originally intended. 

rent = rent.replace(["..", "-"], pd.NA)

numeric_cols = ["count_of_rents", "mean", "lower_quartile", "median", "upper_quartile"]

for col in numeric_cols:
    rent[col] = pd.to_numeric(rent[col], errors="coerce")

rent.head(7)

# ---- Output data ----
# rent: the data frame now cleaned
```

```{python}
#| output: false 
# Step 06: Classify the listings by bedroom category for comparability 
#          with the rental dataset

# This classification allows direct comparison between:
#   - the supply of Airbnb listings by bedroom type, and
#   - the distribution of long-term rental properties by bedroom type.

# It will also enable bedroom-specific analyses of rent gaps .

# Create a function to convert numeric bedroom counts into categorical labels . 
def map_bedroom_category(n):
    if pd.isna(n):
        return "Unknown"
    n = int(n)
    if n <= 0:
        return "Studio"
    elif n == 1:
        return "One Bedroom"
    elif n == 2:
        return "Two Bedrooms"
    elif n == 3:
        return "Three Bedrooms"
    else:
        return "Four or More Bedrooms"

# Apply the classification to the listings dataset
listings_entirehome["bedroom_category"] = listings_entirehome["bedrooms"].apply(map_bedroom_category)

# Review the result
listings_entirehome["bedroom_category"].value_counts()
```

```{python}
#| output: false 
# Step 07: Spatial Join Airbnb revenue estimates by borough shapefile


# Reproject to Britain National Grid
borough_shp = borough_shp.to_crs(epsg=27700)


# Create a subset of listings, with the listings that comply with the conditions to evaluate monthly revenue
# (1) Estimated revenue > 0 - removes listings that are inactive or not generating revenue
# (2) Estimated occupancy 365 > 60 - This threshold removes listings that operate only sporadically or at very low levels of activity. 
#     Filtering them out ensures that the revenue estimates represent listings that are genuinely operating 
#     as part of the short-term rental market and can therefore be compared more meaningfully with long-term rents.

listings_filtered = (
    listings_entirehome[
        (listings_entirehome["estimated_revenue_l365d"] > 0) &
        (listings_entirehome["estimated_occupancy_l365d"] >= 90)  
    ]
).copy()


# Convert into GeoDataFrame
listings_filtered = gpd.GeoDataFrame(
    listings_filtered,
    geometry=gpd.points_from_xy(
        listings_filtered.longitude,
        listings_filtered.latitude
    ),
    crs="EPSG:4326"   # o el CRS original de tus listings
)

# Reproject into British National Grid
listings_filtered = listings_filtered.to_crs(epsg=27700)

# Create estimated monthly revenue (annual revenue divided by 12)
listings_filtered["monthly_revenue"] = listings_filtered["estimated_revenue_l365d"] / 12

#Spatial join: assign each filtered listing to its borough polygon
listings_sjoin = gpd.sjoin(
    listings_filtered,
    borough_shp,
    how="left",
    predicate="within"
)

# Calculate mean monthly revenue by borough and bedroom category
mean_revenue = (
    listings_sjoin
    .groupby(["borough_name", "bedroom_category"])["monthly_revenue"]
    .mean()
    .reset_index()
)

# Convert to wide format (bedroom categories as columns)
mean_revenue_wide = mean_revenue.pivot(
    index="borough_name",
    columns="bedroom_category",
    values="monthly_revenue"
).reset_index()


#Merge revenue table back into the borough shapefile
borough_revenue = borough_shp.merge(
    mean_revenue_wide,
    on="borough_name",
    how="left"
)

# Remove the 'Unknown' bedroom category (not meaningful for comparison)
borough_revenue = borough_revenue.drop(columns=["Unknown"])

# Rename columns for consistency, as we are later adding the revenue of private rent 
borough_revenue = borough_revenue.rename(columns={
    "Studio": "studio_airbnb",
    "One Bedroom": "one_bedroom_airbnb",
    "Two Bedrooms": "two_bedroom_airbnb",
    "Three Bedrooms": "three_bedroom_airbnb",
    "Four or More Bedrooms": "fourplus_airbnb"
})

# Standardise borough name field
borough_revenue = borough_revenue.rename(columns={"borough_name": "borough_clean"})



# Exclude the City of London (we dont have private rental data for this borough)
borough_revenue = borough_revenue[
    ~borough_revenue["borough_clean"] .isin([
        "city and county of the city of london",
        "city of westminster"
    ])
]
borough_revenue.head()
```

```{python}
#| output: false 
# Step 08: Merge rent data with the borough shapefile and the 
#    borough-level Airbnb revenue estimates.
 
# Align long-term rental prices with Airbnb  revenue estimates at the borough level.

# Standardise the borough name to ensure a correct merge
rent["borough_clean"] = rent["borough"].str.lower().str.strip()

# Convert to wide format (bedroom categories as columns)
rent_wide = rent.pivot(
    index="borough_clean",
    columns="bedroom_category",
    values="mean"
).reset_index()

# Drop room category, as we are working with entire units
rent_wide = rent_wide.drop(columns=["Room"])

# Rename columns for consistency, 
rent_wide = rent_wide.rename(columns={
    "Studio": "studio_rent",
    "One Bedroom": "one_bedroom_rent",
    "Two Bedrooms": "two_bedroom_rent",
    "Three Bedrooms": "three_bedroom_rent",
    "Four or More Bedrooms": "fourplus_rent"
})

# Merge the rental data with borough-level Airbnb revenue estimates
revenue_airbnb_rent = borough_revenue.merge(
    rent_wide,
    on="borough_clean",
    how="left")


# Identify all numeric columns that should not be strings
cols_to_fix = [
    "studio_rent", "one_bedroom_rent", "two_bedroom_rent",
    "three_bedroom_rent", "fourplus_rent",
    "studio_airbnb", "one_bedroom_airbnb", "two_bedroom_airbnb",
    "three_bedroom_airbnb", "fourplus_airbnb"
]

# Convert all to numeric (strings → numbers, invalid → NaN)
for col in cols_to_fix:
    revenue_airbnb_rent[col] = pd.to_numeric(
        revenue_airbnb_rent[col], errors="coerce"
    )
    
revenue_airbnb_rent.head()

# ---- Output data ----
# revenue_airbnb_rent : Consolidates the two sources or the housing market at a borough scale:
#       (1) estimated Airbnb monthly revenue, disaggregated by bedroom category
#       (2) mean long-term private rental prices, disaggregated by bedroom category
#       This dataset enables direct comparison between the two categories.

```


{{< pagebreak >}}

## **Figure 1.3** | Scatterplot Airbnb vs Private Rent Monthly revenue by bedroom category
```{python}
#| echo: false
#| fig-align: center
#| fig-width: 10
#| fig-height: 6   

# Step 08: 2x3 Layout (5 plots)
categories = {
    "Studio": ("studio_rent", "studio_airbnb"),
    "1-Bed": ("one_bedroom_rent", "one_bedroom_airbnb"),
    "2-Bed": ("two_bedroom_rent", "two_bedroom_airbnb"),
    "3-Bed": ("three_bedroom_rent", "three_bedroom_airbnb"),
    "4+ Bed": ("fourplus_rent", "fourplus_airbnb")
}

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# 1. 
fig, axes = plt.subplots(2, 3, figsize=(12, 7)) 
axes_flat = axes.flatten()

for i, (cat, (rent_col, airbnb_col)) in enumerate(categories.items()):
    ax = axes_flat[i]
    
    x = pd.to_numeric(revenue_airbnb_rent[rent_col], errors="coerce")
    y = pd.to_numeric(revenue_airbnb_rent[airbnb_col], errors="coerce")

    ax.scatter(x, y, color='#8B0000', s=15, alpha=0.6)

    valid = ~(x.isna() | y.isna())
    x_valid = x[valid]
    y_valid = y[valid]

    if len(x_valid) > 1:
        m, b = np.polyfit(x_valid, y_valid, 1)
        ax.plot(x_valid, m*x_valid + b, color="navy", linewidth=2)

    if len(x_valid) > 0:
        tmin, tmax = min(x_valid.min(), y_valid.min()), max(x_valid.max(), y_valid.max())
        ax.set_xlim(tmin, tmax)
        ax.set_ylim(tmin, tmax)
    
    ax.set_aspect("equal", "box")
    ax.set_title(cat, fontsize=11, fontweight='bold')
    ax.grid(alpha=0.3)


fig.delaxes(axes_flat[5])
plt.suptitle("Airbnb Revenue vs Private Rent by Bedroom Category", fontsize=18)
plt.tight_layout()
plt.show()
```

**Summary Figure 1.3**   

*   **Small Units (Studio/1-Bed)**: Returns are roughly equal between Airbnb and private rent. The points cluster along the diagonal line.

&nbsp;

*   **Large Units (2-4+ Beds)**: The gap widens significantly. Airbnb revenue is much higher than private rent.

&nbsp;

*   **Implication**: This creates a strong financial incentive to convert family-sized homes into short-term rentals, potentially hurting local housing supply.


```{python}
#| output: false 
#  Calculate the slopes to enable precise comparison between the 5 categories
slopes = {}

for cat, (rent_col, airbnb_col) in categories.items():
    
    # Force numeric (avoid dtype errors)
    x = pd.to_numeric(revenue_airbnb_rent[rent_col], errors="coerce")
    y = pd.to_numeric(revenue_airbnb_rent[airbnb_col], errors="coerce")
    
    valid = ~(x.isna() | y.isna())
    x_valid = x[valid]
    y_valid = y[valid]
    
    # Compute slope only if there are enough points
    if len(x_valid) > 1:
        m, b = np.polyfit(x_valid, y_valid, 1)
        slopes[cat] = m
    else:
        slopes[cat] = np.nan

slopes
```

## **Figure 1.4** | Rent Gap (Airbnb vs Private Rent) Two Bedroom Units

```{python}
# Step 10: Map the rent gap (Airbnb revenue / private rent) for two-bedroom units 
#    across London boroughs.
# The rent gap is calculated as the ratio between the average monthly
# Airbnb revenue and average private rent for the same unit size.
# Values > 1 indicate that Airbnb is more profitable than renting 
# long-term, revealing boroughs where incentives to convert homes 
# into short-term rentals are strongest.

# Calculate the rent gap for three-bedroom units
revenue_airbnb_rent["rentgap_three_bedroom"] = (
    revenue_airbnb_rent["three_bedroom_airbnb"] /
    revenue_airbnb_rent["three_bedroom_rent"]
)

# Plot the spatial distribution of the rent gap
revenue_airbnb_rent.plot(
    column="rentgap_three_bedroom",
    cmap="OrRd",
    scheme="Quantiles",
    k=5,
    linewidth=0.5,
    edgecolor="white",
    legend=True,
    figsize=(10,10)
)
plt.title("Rent Gap (Airbnb vs Private Rent) – Two Bedroom Units", fontsize=12)
plt.axis("off")
plt.show()

# ---- Output figure ----
#  Figure 04: Cloropeth map of Rent Gap (airbnb vs private rent) in two-bedroom units
```

**Summary Figure 1.4**  

- This map shows the spatial distribution of the rent gap—defined as the ratio between Airbnb monthly revenue and private rent—for two-bedroom units across London boroughs. The pattern is concentrated in central and high-demand boroughs, suggesting greater market pressure on family-sized housing in these areas.  
- Boroughs are very large areas, and the rent gap can change a lot inside the same borough. So this map shows the general pattern, but not the smaller local differences.  

{{< pagebreak >}}

## **Figure 1.5** | Rent gap in Camden, by bedroom category

```{python}
# Step 11: Zoom in on a high-pressure borough, Camden in this case, to compare 
#     Airbnb revenue and private rent by unit size and illustrate 
#     local rent gaps in more detail.


df = revenue_airbnb_rent

# Choose borough
borough = "camden"   # <--- cambia aquí

# Filter borough row
row = df[df["borough_clean"] == borough].iloc[0]

# Tipologías y columnas asociadas
categories = ["Studio", "One Bedroom", "Two Bedrooms", "Three Bedrooms", "Four+ Bedrooms"]

airbnb_cols = [
    "studio_airbnb",
    "one_bedroom_airbnb",
    "two_bedroom_airbnb",
    "three_bedroom_airbnb",
    "fourplus_airbnb"
]

rent_cols = [
    "studio_rent",
    "one_bedroom_rent",
    "two_bedroom_rent",
    "three_bedroom_rent",
    "fourplus_rent"
]

# Extract values for this borough
airbnb_values = row[airbnb_cols].values
rent_values = row[rent_cols].values

# Plot
x = np.arange(len(categories))
width = 0.35

plt.figure(figsize=(12,8))

plt.bar(x - width/2, airbnb_values, width, label="Airbnb Revenue (£/month)")
plt.bar(x + width/2, rent_values, width, label="Private Rent (£/month)")

plt.xticks(x, categories, rotation=45, fontsize=12)
plt.yticks(fontsize=12)

plt.ylabel("£ per Month", fontsize=12, fontweight='bold')
plt.title("Airbnb vs Private Rent by Unit Size (London Averages)", fontsize=18, fontweight='bold', pad=20)
plt.legend(
    fontsize=12, 
    loc='upper left', 
    facecolor='white',  
    edgecolor='black',  
    framealpha=1,       
    shadow=True         
)
plt.grid(axis="y", alpha=0.3)

plt.tight_layout()
plt.show()



# ---- Output figure ----
#  Figure 05: Scatterplot Airbnb Revenue vs Private Rent by Bedroom Category 
```


**Summary Figure 1.5**  

 - In a high demand , center located neighborhood like Camden, Airbnb earns more than private rent across all unit sizes.  
 -  For two-bedroom homes, Airbnb brings in around £1,000 more per month (about +35%).  
 - For three-bedroom homes, the gap is even larger, Airbnb earns roughly £1,800 extra per month (around +60%).  

Under London’s regulatory framework, a property may not be rented out for more than 90 nights a year as short-term rental without applying for a change of use, from residential to temporary acommodation.  


&nbsp;

::: {.content-hidden}
it would be good to have an approx stat here on the % of airbnbs that are operating with a temporary accomodation license - presumably almost no listings have this
:::

  
  We define short-term rental listings as those which allow a minimum length of stay shorter than 30 nights.  

&nbsp;

::: {.content-hidden}
It would be great to find a stat here that says x % of airbnbs are used for short term rather than long term rentals to back up our assumptions 
:::

{{< pagebreak >}}

Although the Inside Airbnb dataset provides calendar data showing the availability of a listing for the next 365 days, it is impossible to assertain whether unavailable nights are as a result of nights being booked out by a guest or nights being 'blacked-out' by the owner. Thus, information on the number of days that listings are being rented out annually is sparse and unreliable. Instead, the following calculation was used to estimate the number of nights that each listing was occupied in the last 12 months:
&nbsp;

::: {.content-hidden}
we will make this into a nice equation on quarto
occupancy in the last 12 months = (number of reviews in the last 12 months)/(percentage of guests that leave reviews)*(average minimum number of nights that guests are able to book for)
:::

$$
O = \frac{R}{r} \times N_{min}
$$

**Where:**

*   $O$: Estimated Occupancy Rate (last 12 months)
*   $R$: Total Number of Reviews (last 12 months)
*   $r$: Review Rate (assumed percentage of guests who leave a review, typically 50%)
*   $N_{min}$: Average Minimum Nights per stay

This formula assumes a standard review conversion rate to estimate total bookings from visible reviews.



::: {.content-hidden}
#### **Assumptions**
* number of reviews in the last 12 months is a proxy for demand (@quattrone2016)  
* 70% of guests leave reviews (@quattrone2016)  
* minimum number of nights is a number inputted by the owner on the app, and this can changed at any time and on a day-to-day basis (based on seasonality, preference,  etc) so might not reflect the situation for the previous 12 months  
* Some of calculated last 12 month occupancy was > 365 (possibly because the current minimum number of nights is higher than the true minimum number of nights). We assume that it is plausible to assume 
:::

```{python}
#| output: false 
# -- Methodology: Illegal listings and revenue --
# Steps Overview:
# 1. Create a function to calculate estimated occupancy, revenue illegal and revenue illegal revenue per listing
#        *** Output *** 
#        Stat: Estimated illegal revenue earned in the last year
#        Stat: Percent of illegal revenue from entire homes
# 2. Calculate the number (and %) of hosts that contribute to 50% of total illegal revenue
#        *** Output *** 
#        Stat: Number of hosts contributing to 50% of illegal revenue
# 3. Calculate where majority of the illegal revenue is being generated
#        *** Output *** 
#        Stat: Number of boroughs in which 80% of illegal rebenue is generated, and how much of this is in central london
```

```{python}
#| output: false 
# ------ Step 1: Function to calculate estimated, occupancy, revenue and illegal revenue under different inputs ------------

def calculate_occupancy_and_revenue(
    df,
    min_cutoff, 
    review_rate, 
    min_stay_buffer,
    max_occupancy):

    """
    Calculate occupancy, total revenue and illegal revenue for each listing.
    """

    # -- Step 1.1 --
    # Create a new dataframe (listings_illegal)
    # subset it to the minimum cutoff
    # listings_illegal is a subset of listings which only includes listings where the minimum minimum nights are < 30 days
    # this is taken as an indicator that the listing is available for short term rentals
    listings_illegal = df[df["minimum_minimum_nights"] < min_cutoff].copy()
    
    # -- Step 1.2 --
    # Calculate estimated occupancy for the last twelve months --> occupancy_ltm
    # We use the minimum_minimum_nights (inputted by the host) here as conservative estimate for the minimum number of days that the listing was occupied in the last 12 months
    # Hosts have general restrictions for the minimum number of nights one can stay at the listing (minimum nights)
    # But these restrictions can be changed by the host so that certain periods have different minimum night restrictions
    # In reality, the number of nights each guest is staying is likely higher than the minimum_minimum_nights
    # This can be controlled by th min_stay_buffer
    listings_illegal["occupancy_ltm"] = (listings_illegal["number_of_reviews_ltm"] / review_rate) * (listings_illegal["minimum_minimum_nights"] + min_stay_buffer)
    
    # Some of the calculated estimated occupancies mayb be very high (with some > 365 which doesn't make sense for an annual estimate)
    # This is likely due to the unreliablitty of 1. number of reviews being a proxy for number of bookings, and 2. minimum_minimum_nights being a proxy for minimum length of stay
    # To account for this, we set a cutoff for the occupancy
    listings_illegal["occupancy_ltm"] = listings_illegal["occupancy_ltm"].clip(upper=max_occupancy)

    # -- Step 1.3 --
    # Calculate estimated annual revenue per listing for the last twelve months
    # It is important to note here that the number of listings without data on price is extremely high
    listings_illegal["revenue_ltm"] = listings_illegal["occupancy_ltm"] * listings_illegal["price"]

    # -- Step 1.4 --
    # Calculate estimated annual illegal revenue per listing (ie only the revenue earned after 90 day limit was exceeded)
    listings_illegal["illegal_revenue_ltm"] = (
    (listings_illegal["occupancy_ltm"] - 90).clip(lower=0)
    * listings_illegal["price"])

    # -- Step 1.5 --
    # Calculating stats
    # Calculating the percentage of illegal listings out of the total
    illegal_rev_count = listings_illegal[listings_illegal["illegal_revenue_ltm"] > 0].shape[0]  # Number of listings for which illegal revenue could be calculated
    rev_count = listings_illegal[listings_illegal["revenue_ltm"] > 0].shape[0]     # Number of listings for with revenue could be calculated
    per_illegal_listings = (illegal_rev_count/rev_count)*100

    # Total illegal airbnb revenue
    illegal_rev_sum = listings_illegal["illegal_revenue_ltm"].sum(skipna=True)

    # Calculating the percentage of illegal listings out of the total
    illegal_rev_for_entire = listings_illegal.loc[
        listings_illegal["room_type"] == 'Entire home/apt',
        'illegal_revenue_ltm'].sum(skipna=True)
    per_illegal_rev_entire = (illegal_rev_for_entire/illegal_rev_sum)*100

    # Other stats (not used but may be relevant)
    # Total number of listings within the subset
    total_count = listings_illegal.shape[0]

    no_reviews = listings_illegal[listings_illegal["number_of_reviews_ltm"] == 0].shape[0]
    # Number of listings for which occupancy could be estimated
    occ_count = listings_illegal[listings_illegal["occupancy_ltm"] > 0].shape[0]

    # Number of listings for which illegal occupancy was calculated
    illegal_occ_count = listings_illegal[listings_illegal["occupancy_ltm"] > 90].shape[0]
    
    # Total airbnb revenue
    rev_sum = listings_illegal["revenue_ltm"].sum(skipna=True)
    
    return listings_illegal, per_illegal_listings, illegal_rev_sum, per_illegal_rev_entire
```

```{python}
# ------ Step 2: Function to calculate the proportion of illegal hosts making up 50% of total illegal revenue ------------

def calculate_illegal_host_distribution(listings_illegal):
    """
    Step 2: Calculate the number (and %) of hosts that contribute to 50% of total illegal revenue.
    """

    # -- Step 2.1 --
    # Create a subset for only illegal listings
    listings_illegal_only = listings_illegal[listings_illegal["illegal_revenue_ltm"] > 0].copy()

    # -- Step 2.2 --
    # Group by host ID and calculate each host's illegal revenue
    illegal_revenue_by_host = (
        listings_illegal_only
        .groupby("host_id", as_index=False)
        .agg(
            host_illegal_revenue=("illegal_revenue_ltm", "sum"),
        )
    )
    number_illegal_hosts = illegal_revenue_by_host.shape[0]

    # -- Step 2.3 --
    # Calculate the number of hosts that make up the top 50% of revenue
    sorted_illegal_hosts = illegal_revenue_by_host.sort_values(
        by="host_illegal_revenue", ascending=False
    )

    # -- Step 2.4 --
    # Calculate cumulative sum of illegal revenue
    sorted_illegal_hosts["cum_sum"] = sorted_illegal_hosts["host_illegal_revenue"].cumsum()

    # -- Step 2.5 --
    # Calculate cumulative percentage
    total = sorted_illegal_hosts["host_illegal_revenue"].sum()
    sorted_illegal_hosts["cum_pct"] = sorted_illegal_hosts["cum_sum"] / total

    # -- Step 2.6 --
    # Count how many hosts are needed to reach 50%
    # (+1 ensures we include the host that crosses the threshold)
    num_ill_hosts = (sorted_illegal_hosts["cum_pct"] <= 0.50).sum() + 1

    return number_illegal_hosts, num_ill_hosts
```

```{python}
#| output: false 
# ------ Step 3: Function to calculate where 80% of illegal revenue is happeneing by borough ------------

def calculate_illegal_revenue_distribution_by_borough(listings_illegal, borough_shp):
    """
    Step 2: Calculate in which boroughs 80% of illegal revenue occurs, and how many of these are in inner london.
    """
    
    # -- Step 3.1 --
    #Convert listings fulltime df into Geopandas

    listings_illegal = gpd.GeoDataFrame(
        listings_illegal,
        geometry=gpd.points_from_xy(
            listings_illegal.longitude,
            listings_illegal.latitude
        ),
        crs="EPSG:4326"
    )
    
    #Reproject to Britain National Grid
    listings_illegal = listings_illegal.to_crs(epsg=27700)

    # -- Step 3.2 --
    # Spatial join of the listing according to borough and create a group count value for each borough
    listings_sjoin = gpd.sjoin(
        listings_illegal,
        borough_shp,
        how="left",
        predicate="within"
    )

    # -- Step 3.3 --
    # Group by borough and calculate the number of illegal listings and their total revenue
    illegal_stats_borough = (
        listings_sjoin
        .groupby("borough_code")
        .agg(
            listing_count=("illegal_revenue_ltm", lambda x: (x > 0).sum()),
            illegal_revenue_sum=("illegal_revenue_ltm", "sum")
        )
        .reset_index()
    )

    # -- Step 3.4 --
    #   Merge the listing_count values (by borough) into the borough GeoDataFrame
    borough_shp_illegal = borough_shp.merge(
        illegal_stats_borough,
        on="borough_code",
        how="left"
    )

    # SAFETY CHECK
    if "illegal_revenue_sum" not in borough_shp_illegal.columns:
        raise ValueError("illegal_revenue_sum column missing after merge.")

    # -- Step 3.5 --
    # Calculate the number of boroughs that make up 80% of illegal revenue
    
    # Sort boroughs by illegal_revenue_sum descending
    sorted_boroughs = borough_shp_illegal.sort_values(
        by="illegal_revenue_sum", ascending=False
    )

    # Calculate cumulative sum of illegal revenue
    sorted_boroughs["cum_sum"] = sorted_boroughs["illegal_revenue_sum"].cumsum()

    # Calculate cumulative percentage
    total = sorted_boroughs["illegal_revenue_sum"].sum()
    sorted_boroughs["cum_pct"] = sorted_boroughs["cum_sum"] / total

    # Count how many boroughs are needed to reach 80%
    num_boroughs_80pct = (sorted_boroughs["cum_pct"] <= 0.80).sum() + 1

    # Check how many of these boroughs are in inner london
    inner_london = [
    "camden",
    "greenwich",
    "hackney",
    "hammersmith and fulham",
    "islington",
    "kensington and chelsea",
    "lambeth",
    "lewisham",
    "southwark",
    "tower hamlets",
    "wandsworth",
    "city of westminster",
    "city of london"
]
    top_boroughs = sorted_boroughs.iloc[:num_boroughs_80pct]["borough_name"].tolist()
    inner_overlap = [b for b in top_boroughs if b in inner_london]
    num_inner_london = len(inner_overlap)
    
    # Return all useful outputs
    return num_boroughs_80pct, num_inner_london
```

```{python}
#| output: false 
param_sets = [
    {   # SET 1: Conservative estimate
        "min_cutoff": 30, # we are only considering short-term rentals to be those which are rented out for less than 30 days at a time
        "review_rate": 0.70, # We assume that 70% of Airbnb guests leave a review based on the findings of previous studies (Fradkin 2015)
        "min_stay_buffer": 0, # since this is a conservative estimate, we assume that guests which left a review only stayed the minimum number of nights allowed by the host
        "max_occupancy": 292 # we assume a maximum of 80% occupancy for the year
    },
    {   # SET 2: Less conservative estimate
        "min_cutoff": 30,
        "review_rate": 0.60, # fewer people leave reviews (only 60%)
        "min_stay_buffer": 1,  # people stay one night longer than the minimum nights
        "max_occupancy": 292
    }
]

for i, params in enumerate(param_sets, start=1):
    
    # --- Run function 1 (illegal revenue) ---
    listings_illegal, per_illegal_listings, illegal_rev_sum, per_illegal_rev_entire = (
        calculate_occupancy_and_revenue(listings, **params)
    )

    print(f"\n------ Results for parameter set {i} ------")
    print(f"Percent illegal listings out of total: {per_illegal_listings:,.0f}")
    print(f"Total illegal revenue: {illegal_rev_sum:,.0f}")
    print(f"Percentage of illegal revenue from entire home: {per_illegal_rev_entire:,.0f}")

    # --- Run function 2 (illegal revenue distribution among hosts) ---
    number_illegal_hosts, num_ill_hosts = calculate_illegal_host_distribution(listings_illegal)
    
    print(f"Only {(num_ill_hosts/number_illegal_hosts)*100:,.0f}% of hosts involved in illegal Airbnb activity make up 50% of all illegal Airbnb revenue")

    # --- Run function 3 (illegal revenue distribution among boroughs) ---
    num_boroughs_80pct, num_inner_london = calculate_illegal_revenue_distribution_by_borough(listings_illegal, borough_shp)

    print(f"Only {num_boroughs_80pct} boroughs generated 80% of the illegal revenue, {num_inner_london} of which are Inner London boroughs.")
```

#### **Illegal activity**  

Under London’s current regulations, a residential property may not be rented out for more than 90 nights per year as a short-term rental without first obtaining planning permission for a “material change of use” from residential to temporary accommodation. We define illegal listings as those which allow short-term rentals (rentals of less than 30 days) and exceed this 90 day limit.   

Highly conservative estimates show that one in every **xxx** listings is involved in illegal Airbnb activity, with a minimum of £16 million in illegal revenue earned in the last year. More interestingly, **xxx**

50% of the illegal revenue is attributable to only 171 individual hosts (5% of illegal hosts). Illegal Airbnb activity is mainly localised in the city centre, with 80% of illegal activity occurring in only nine boroughs, all of which are located in the city centre. Although illegal activity generates a significant amount of revenue (and this is likely to be much higher), it is concentrated among a small number of hosts and locations, meaning the overall impact across the city is limited.

{{< pagebreak >}}

# **Question 2: How many professional landlords are there?**

Following the literature (-@shabrina2022), a professional landlord can be defined by making these three assumptions:  

- A professional landlord rents entire homes or apartments.  
- A professional landlord owns listings with a high availability per year.  
- A professional landlord owns more that 1 listings, since managing 2 or more listings requires time and coordination, which makes it unlikely for this to be just a "side-job".  

The filter for landlords renting entire homes/apartments and with high availability, so the next step if to filter multi-listers hosts to get the final count of **professional landlords**.  


## **Figure 2.1** | Histogram of number of listings per professional host

```{python}
#| output: false 
# Identify the total number of professional hosts
# Filter the multi-listers from listings with high availability per year that are entire homes or apartments  

host_counts = listings_fulltime["host_id"].value_counts()
prof_land = host_counts[host_counts > 1]

print(f"There are {host_counts.count()} in total, and {prof_land.count()} are professional landlords")

# ---- Output Data -----
# Total number of professional landlords
# host_counts = count of the number of listings per landlord
# prof_land = number of professional landlords 
```

```{python}
# Create a Histogram explaining the frequency of the number of hosts with the number of listings per host
# Using a log scale for a better visualization 

bins = list(range(0, 500, 15)) #Define bins

plt.figure(figsize=(10,5))

# 1. 
plt.hist(prof_land.values, 
         bins=bins, 
         edgecolor='white', 
         color="#404040") 

plt.yscale('log')

ax = plt.gca() 

ax.spines['top'].set_visible(False) 
ax.spines['right'].set_visible(False) 

# 2. 
ax.set_xlabel("NUMBER OF LISTINGS PER HOST", 
              loc='left', 
              fontsize=10, fontweight='bold')
ax.set_ylabel("FREQUENCY OF HOSTS (LOG)", 
              loc='top', 
              fontsize=10, fontweight='bold')

# 3. 标
ax.set_title("Histogram of Listings per Professional Host", 
             fontsize=12, fontweight='bold', pad=15)

# 4. 
ax.tick_params(axis='both', labelsize=10)

plt.tight_layout()
plt.show()

# Histogram of the distribution of the number of listings per professional landlord
```

&nbsp;

&nbsp;

&nbsp;


According to our assumptions, there are **15 050 professional hosts**, and it looks like most of them have less than 100 listings. The hosts with more listings are excepmtions, but it's still important to include them in the analysis since these cases are a signal that the system is, as mentioned before, "out of control".

## **Figure 2.2** | Distribution of total estimated revenue of professional hosts

```{python}
#| output: false 
# Calculate the amount of revenue per listing in a year
# Extract the professsional landlord from listings_fulltime

prof_ids = prof_land.index 

# Define columns to keep
cols_prof = ['id','host_id','host_is_superhost','estimated_revenue_l365d','geometry']

# Make the filter
listings_prof = listings_fulltime.loc[
    listings_fulltime["host_id"].isin(prof_ids),
    cols_prof].copy()

# Plot
listings_prof

# ---- Output Data -----
# A dataframe with the listings of the professional landlords only and the estimated revenue of each listing per year
```


```{python}
#| output: false 
# Calculate the amount of revenue per host in one year
# Sum revenue per professional host in pounds £

prof_rev = (listings_prof.groupby("host_id")
    .agg(n_listings=("id", "nunique"),
         is_superhost=("host_is_superhost", "max"),
         total_est_revenue=("estimated_revenue_l365d", "sum")
    )
    .reset_index()
    .sort_values(by="total_est_revenue", ascending=False))

# ---- Output Data -----
# prof_rev: ddataframe including the total revenue per professional landlord
```


```{python}
# Histogram explaining the distribution of the total estimated revenue per professional landlord
# Using a log scale for a better visualization 

bins = list(range(0, 6000000, 200000)) #Define bins

plt.figure(figsize=(10,5))

data = prof_rev['total_est_revenue'] #Ignore 0 values
data = data[data > 0]


plt.hist(data.values, 
         bins=bins, 
         edgecolor='white', 
         color="#8B0000") 

plt.yscale('log')

ax = plt.gca() 


ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, pos: f"{int(x):,}".replace(",", " ")))


ax.spines['top'].set_visible(False) 
ax.spines['right'].set_visible(False) 


ax.set_xlabel("TOTAL ESTIMATED REVENUE PER HOST", 
              loc='left', 
              fontsize=10, fontweight='bold')
ax.set_ylabel("FREQUENCY OF HOSTS (LOG)", 
              loc='top', 
              fontsize=10, fontweight='bold')


ax.set_title("Distribution of Total Estimated Revenue (Professional Hosts)", 
             fontsize=12, fontweight='bold', pad=15)


ax.tick_params(axis='both', labelsize=10) 

plt.tight_layout()
plt.show()

# ---- Output Data -----
# A histogram of the distributions of the revenues per professional landlord
```

&nbsp;

&nbsp;

&nbsp;

**Following from the Output:** 

Knowing which ones are the professional landlords would help deepen into finding how do they operate in the system. A way to do this is by:  

- Calculating the estimated revenue per host in a year  
- Plotting the spatial distribution of the top 6 professional hosts by number of listings  
- Finding if the number of listings per professional host is correlated with being a "superhost"(@airbnbhelpcenter), a sign of high-quality service  

&nbsp;

&nbsp;

These outcomes help reveal whether their activity is driven by commercial and profit-oriented practices.  

Requirements to be a Superhost according to Article:   

- Experience: At least 10 completed stays (or three stays totalling 100+ nights) which is a sign of consistency  
- Responsive: Responds quickly to messages (90% response rate) -> a sign of commitment  
- Three-month criteria: To qualify, these criteria are checked every three months  

{{< pagebreak >}}

## **Figure 2.3** | Spatial distribution of the listings of the top6 professional landlords with the most listings

```{python}
#| echo: false
#| fig-width: 8      
#| fig-height: 11    
#| out-width: "100%" 
# Plot the spatial distribution of the top 6 professional hosts by number of listings.

# Picking the top 6 professional hosts random hosts to plot
# Keep the information about number of listings and total estimated revenue

prof_top6 = (prof_rev
    .set_index("host_id")
    .sort_values(by="n_listings", ascending=False)
    .head(6)                            
)

#Extract the spatial attributes of the top 10 professional hosts
listings_prof6 = listings_prof[listings_prof["host_id"]
    .isin(prof_top6.index)].copy()

import matplotlib.pyplot as plt
fig, axes = plt.subplots(3, 2, figsize=(10, 13))

# To make sure all the maps share the same frame
xmin, ymin, xmax, ymax = borough_shp.total_bounds

# Loop for each of the top 6 hosts for subplots
for ax, (host_id, row) in zip(axes.flat, prof_top6.iterrows()):
    n_list = row["n_listings"]
    revenue = row["total_est_revenue"]
    
    # subset listings for this host
    sub = listings_prof6[listings_prof6["host_id"] == host_id]

    # 1. mapping
    borough_shp.plot(
        edgecolor="#13132bff", 
        facecolor="white",
        linewidth=0.5,
        ax=ax
    )
    
    # 2. scatter
    sub.plot(
        ax=ax,
        markersize=2,       
        color="#810000ff",    
        alpha=0.8,
        legend=False
    )

    # 3. title
    ax.set_title(
        f"Host ID {host_id}\n"
        f"Listings: {n_list} | Rev: £{revenue:,.0f}", 
        fontsize=10,
        fontweight='bold', 
        color='#272727ff'   
    )
    
    ax.set_xlim([xmin, xmax])
    ax.set_ylim([ymin, ymax])
    
    ax.set_axis_off() 

plt.subplots_adjust(
    wspace=0.05, 
    hspace=0.4,  
    left=0.01,   
    right=0.99,  
    top=0.95,    
    bottom=0.01  
)

# plt.tight_layout()  
plt.show()
# ---- Output Data -----
# Spatial distribution of the top 6 professional landlords with the most number of listings
```

{{< pagebreak >}}

#### **Figure 2.4** | Correlation between the superhost condition and the number of listings

```{python}
# Finding if the number of listings per professional host is correlated with being a "superhost"
# Create a strip plot

import seaborn as sns

plt.figure(figsize=(10,5))

mi6_palette = {False: "#720000ff", True: "#000042ff"} 

sns.stripplot(
    data=prof_rev,
    x="n_listings",     
    y="is_superhost",    
    hue="is_superhost",
    alpha=0.6,          
    size=3,             
    palette=mi6_palette, 
    orient="h"
)

ax = plt.gca()


ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)


ax.set_xlabel("NUMBER OF LISTINGS PER HOST", loc='center', fontsize=10, fontweight='bold')
ax.set_ylabel("SUPERHOST CONDITION", loc='center', fontsize=10, fontweight='bold')


ax.tick_params(axis='both', labelsize=10)

plt.gca().legend_.remove()
ax.set_title("Correlation between the superhost condition and the number of listings")
ax.tick_params(axis='both', labelsize=18) #Fontsize for axis numbers

# plt.show()

# ---- Output Data -----
# A boxplot of the distribution of the the number of listings under the superhost condition per professional landlord

from scipy.stats import pearsonr
corr_coef, p_value = pearsonr(prof_rev['n_listings'], prof_rev['is_superhost'])


if p_value < 0.001:
    p_value_text = "p < 0.001"
elif p_value < 0.05:
    p_value_text = "p < 0.05"
else:
    p_value_text = f"p = {p_value:.3f}"
```


&nbsp;
&nbsp;



::: {.callout-important icon="false"}
## **INTELLIGENCE BRIEF: STATISTICAL CORRELATION**

**TARGET**: Correlation between *Superhost Status* and *Listings Count*.

*   **Pearson Coefficient ($r$)**: **`{python} f"{corr_coef:.3f}"`**
*   **Significance Level ($p$)**: **`{python} p_value_text`**

**TACTICAL ASSESSMENT**:
The correlation is negligible (close to zero), suggesting Superhost status is **independent** of portfolio size. However, the result is statistically significant ($p < 0.05$), confirming this pattern is not random noise.
:::

&nbsp;

&nbsp;


- The plots show that the top 6 professional landlords are spread across London, with some clustered in central areas and high-value boroughs. This spatial distribution, along with the portfolio size and total estimated revenue for all the listings, indicates that some of these hosts are actually commercial operators, not just individuals sharing their homes as Airbnb was initially intended.  

- In simple terms, our analysis shows that having many Airbnb listings does not mean a host provides better service. This means that having an extensive portfolio does not necessarily indicate higher-quality service, and that commercial, very profitable listings drive the number of listings, as the analysis shows.  


{{< pagebreak >}}

# **Question 3: How many properties would be affected by the opposition’s proposal?**  

This question looks at **professional full-time entire-home Airbnb listings only**, attaches each listing to its borough’s and accordingly the average council tax, in order evaluate several listings affected by the oppsional propusal of increasing the council tax, we are applting a **40% council tax increase** as a stndard value of growth (140%) to estimate how many properties see a minor 10% drop in profit.  

**The workflow:** clean and merge council tax with borough polygons, use filtered listings, spatially join listings to boroughs, compare average annual revenue vs average council tax by borough, then simulate a 40% tax rise and calculate profit loss per listing before aggregating to borough level.  

::: {.content-hidden}
Datasets used: MAYBE MOVE THIS  

1. Professional landlord listings (from Q2)  
    - Issues  
        - Not all listings have an estimated annual revenue, so they are removed from the calculation.  
        - Not all listings are profitable from the beginning, so once subtracting council tax,they aren't profitable. And removed from analysis because they have negative values. 

2. Borough boundaries (spatial file)  
    - Issues   
        - The boundaries should be deselected and have an output for the actual borough boundary to merge with council tax.  

3. Borough-level council tax     
    - Issues  
        - The file has been selected by the year 2024-2025 and only filtered to Band D, which the GLA references as a starting point to evaluate the overall borough tax.  
        - The value that we extract is associated with the annual tax *(column Band D, which we later rename as council tax).*  
:::

```{python}
#| output: false 
# -- The Work Mythology For evaluating How many properties would be affected by the opposition’s proposal:--
# Steps Overview:
# 1. Cleaning and aligning the Council Tax CSV to prepare for a merge.
# 2. Merge Council Tax Into Borough Shapefile.
# 3. Cleaning and Filtering Listings according to mythology to locate a Professional Landlord, per Q1.
# 4. Spatial Join Listings to Borough Tax Data.
# 5. Calculate Borough Averages: of Airbnb annual Revenue vs Council Tax.
# 6. Evaluate Average Airbnb Revenue vs Average Council Tax per Borough. 
#    Output: Figure 01
# 7. Create a Scenario that increases Council Tax by 40% for Professional Hosts, with the increase varying by borough.
# 8. The borough's share of listings is losing >40% of its profit.
# 9. Linear Relationship Between Revenue and Profit Loss.
#    Output:  **Figure 02** 
# 10. Map of Average Profit Loss per Borough.
#    Output: **Figure 03**
```

```{python}
#| output: false 
# Step 01 - Cleaning and Aligning Council Tax CSV to prepare for merge

# Rename columns in council tax for easy reading
council_tax2 = council_tax.rename(columns={
    'local_authority': 'borough_name_raw',
    'band_d': 'council_tax',
    'code': 'borough_code'})

# Make sure the borough code is a string
council_tax2['borough_code'] = (
    council_tax2['borough_code']
    .astype(str)
    .str.strip())

# make sure council_tax is numeric, (what was yearly band D)
council_tax2['council_tax'] = pd.to_numeric(council_tax2['council_tax'], errors='coerce')

# drop rows where council_tax is missing value 
council_tax2 = council_tax2.dropna(subset=['council_tax'])

# Print Output of row columns and top 5 head of the rows
print("clean council tax:", council_tax2.shape)
display(council_tax2.head())

# ---- Output Data -----
# council_tax2 = Which is a cleaned and renamed version of council_tax
```

```{python}
#| output: false 
# Step 02 - Merge Council Tax Into Borough Shapefile

# Merge the shape file borough_shp with council_tax2 through a left join with the borough code value as a shared id
borough_tax = borough_shp.merge(
    council_tax2[['borough_code', 'council_tax']],
    on='borough_code',
    how='left'
)

print("borough_tax merged:", borough_tax.shape)
print("missing council tax:", borough_tax['council_tax'].isna().sum()) # make sure that all boundries have been attributed council tax
display(borough_tax.head())

# ---- Output Data -----
# borough_tax = Merged spatial file with council tax data
```

```{python}
#| output: false 
# Step 03 - Use listings_filtered = from Q1
# Using the "Inside Airbnb" estimated model and tagging professional landlords

# Start from Q1 output
listings_q3 = listings_filtered.copy()

# 1. Drop rows with missing IDs (extra safety)
before_id_drop = len(listings_q3)
listings_q3 = listings_q3.dropna(subset=['id', 'host_id'])
after_id_drop = len(listings_q3)
print(f"Removed {before_id_drop - after_id_drop} rows with missing id/host_id")

# 2. Drop rows with missing coordinates
before_coord_drop = len(listings_q3)
listings_q3 = listings_q3.dropna(subset=['latitude', 'longitude'])
after_coord_drop = len(listings_q3)
print(f"Removed {before_coord_drop - after_coord_drop} rows with missing coordinates")

# 3. Create NEW COLUMN: host_is_pro
# Professional landlord = more than 1 listing
listings_q3['host_is_pro'] = listings_q3['host_total_listings_count'] > 1

# 4. Filter to professional landlords only (Q3 requirement)
listings_q3 = listings_q3[
    (listings_q3['host_is_pro'] == True)
].copy()

print("Professional full-time entire homes (estimated model):", listings_q3.shape)
display(listings_q3.head())

# ---- Output ----
# listings_q3 = cleaned + new column + filtered to pros
```

```{python}
#| output: false 
listings_q3.columns
```

```{python}
#| output: false 
# Step 04 - Spatial Join Listings to Borough Tax Data (clean)

# Make sure borough_tax CRS 
borough_tax = borough_tax.to_crs(epsg=4326) 

# Filter listings_fulltime to the London bounding box
# First, calculate the bounding box by maximum and minimum values of the borough boundaries
minx, miny, maxx, maxy = borough_tax.total_bounds

in_london_bbox = (
    (listings_q3['longitude'] >= minx) &
    (listings_q3['longitude'] <= maxx) &
    (listings_q3['latitude']  >= miny) &
    (listings_q3['latitude']  <= maxy)
)

print("full-time listings total:", len(listings_q3))
print("full-time listings inside London bbox:", in_london_bbox.sum())

# make sure all listings point are within the bounding box
listings_london = listings_q3[in_london_bbox].copy()

# Create GeoDataFrame from these listings within the London bounding box
listings_gdf = gpd.GeoDataFrame(
    listings_london.copy(),
    geometry=gpd.points_from_xy(
        listings_london.longitude,
        listings_london.latitude
    ),
    crs="EPSG:4326"
)

# Spatial join using within the same as in the previous methods 
# taking the borough geometry and evaluating which points are within which boroughs, 
# and assigning to each point its borough value and accordingly its annual council tax.
listings_gdf = gpd.sjoin(
    listings_gdf,
    borough_tax[['borough_code', 'borough_name', 'council_tax', 'geometry']],
    how='left',
    predicate='within'
)

print("after spatial join:", listings_gdf.shape)
print("listings missing council tax after join:",
      listings_gdf['council_tax'].isna().sum())

# drop unmatched listings
# points that weren't matched with a borough would be lost
rows_before = listings_gdf.shape[0] 
listings_gdf = listings_gdf.dropna(subset=['council_tax']).copy()
rows_after = listings_gdf.shape[0]
print("dropped unmatched listings:", rows_before - rows_after)
print("after dropping unmatched listings:", listings_gdf.shape)

display(listings_gdf.head())

# Quick visual check
# ax = borough_tax.plot(edgecolor='black', facecolor='none', figsize=(6, 6))
listings_gdf.sample(1000, random_state=1).plot(ax=ax, markersize=1)

# ---- Output Data -----
# listings_london = only points located inside the London Bounding Box
# listing_gdf = geo-dataframe containing listing points and their attributes, boroughs, and council tax.
```

```{python}
#| output: false 
listings_gdf.columns
```

```{python}
#| output: false 
# Step 05 -  Borough Averages: Revenue vs Council Tax

# Create a Copy of the file just for this Summary (Safety Reasons)
listings_gdf = listings_gdf.to_crs(epsg=27700) 

listings_for_summary = listings_gdf.copy()

# Group the listings by Borough, calculate new values for the number of listings per borough, Average annual revenue, and average council tax per borough. 
borough_summary = (
    listings_for_summary
    .groupby(['borough_code', 'borough_name'], as_index=False)
    .agg(
        avg_airbnb_revenue=('estimated_revenue_l365d', 'mean'),  # average revenue
        avg_council_tax=('council_tax', 'mean'),        # average council tax
        n_listings=('id', 'count')                      # number of listings
    )
    .sort_values('avg_airbnb_revenue', ascending=False) # affects the plot
)

print("borough_summary shape:", borough_summary.shape)
display(borough_summary.head())


# ---- Output Data -----
# borough_summary = is a summarized table of revenue,tax, and listing per borough.
```


## **Figure 3.1** | Average Airbnb Revenue vs Average Council Tax per Borough 

```{python}
# Step 06 - Figure 01
# A combined line and point graph comparing:
#   (1) Average annual Airbnb revenue per borough
#   (2) Average council tax per borough (Band D)
#   (3) Number of professional full-time listings per borough 
# The x-axis is ordered by borough_summary (currently sorted by number of listings).

# Prep the x-axis labels and positions
# Convert borough names to Title for neat labels.
x_labels = borough_summary['borough_name'].str.title().tolist()

# Create a numeric position for each borough on the x-axis.
x_pos = np.arange(len(x_labels))

# Extract the series we want to plot from the summary table:
rev_vals = borough_summary['avg_airbnb_revenue'].values   # average annual revenue per borough
tax_vals = borough_summary['avg_council_tax'].values      # average council tax per borough
n_vals   = borough_summary['n_listings'].values           # number of listings per borough

# Create the base figure and first axis (for revenue)
fig, ax1 = plt.subplots(figsize=(14, 8))

# Plot Line 1: Average Airbnb revenue per borough (left y-axis)
rev_line = ax1.plot(
    x_pos,
    rev_vals,
    marker='o',
    color='tab:purple',
    label='Avg annual Airbnb revenue.'
)[0]

# Label the left y-axis for revenue and set the borough names on the x-axis.
ax1.set_ylabel('Average annual revenue (£)')
ax1.set_xticks(x_pos)
ax1.set_xticklabels(x_labels, rotation=90)

# Create the second axis (for council tax) sharing the same x-axis
ax2 = ax1.twinx()

# Plot Line 2: Average council tax per borough (right y-axis)
tax_line = ax2.plot(
    x_pos,
    tax_vals,
    marker='s',
    linestyle='--',
    color='tab:orange',
    label='Avg council tax (Band D)'
)[0]

# Label the right y-axis for council tax.
ax2.set_ylabel('Average council tax (£)')

# Add the number of listings as text labels above the revenue points
# Calculate a small vertical offset so the labels sit just above the revenue markers.
offset = (rev_vals.max() - rev_vals.min()) * 0.04  # 4% off the revenue range

for x, y, n in zip(x_pos, rev_vals, n_vals):
    ax1.text(
        x,
        y + offset,
        str(n),            # show the count of listings
        ha='center',
        va='bottom',
        fontsize=12,
        rotation=90
    )

# Combined legend for both lines
ax1.legend(
    [rev_line, tax_line],
    ['Avg annual Airbnb revenue', 'Avg council tax (Band D)'],
    loc='upper right'
)

# Final layout and title 
plt.title(
    'Average Airbnb Revenue vs Average Council Tax per Borough\n'
    '(Professional full-time listings)',
    fontsize=18,       
    pad=15             
)

plt.tight_layout()
# plt.show() 

# ---- Output Figure ----
# Figure 01: Dual-axis line graph showing how average Airbnb revenue and average council tax
#            vary by borough, with listing counts labelled above each revenue point.
```

```{python}
#| output: false 
# Step 06b - Text Summary for Figure 01
# A short summary of:
#   (1) Average annual revenue per borough
#   (2) Average council tax per borough
#   (3) Top / bottom revenue boroughs
#   (4) Correlation between revenue and council tax

# Ensure we use the same ordering as Figure 01 (highest to lowest revenue)
borough_summary_rev = (
    borough_summary
    .sort_values('avg_airbnb_revenue', ascending=False)
    .reset_index(drop=True)
)

# 1. Overall averages across all boroughs
overall_rev_mean = borough_summary_rev['avg_airbnb_revenue'].mean()
overall_tax_mean = borough_summary_rev['avg_council_tax'].mean()

# 2. Top 3 and bottom 3 boroughs by average annual revenue
top3 = borough_summary_rev.head(3)
bottom3 = borough_summary_rev.tail(3)

# 3. Simple Pearson correlation between revenue and council tax
#    (Measures whether high-tax boroughs also tend to have high average revenue.)
corr_rev_tax = borough_summary_rev['avg_airbnb_revenue'].corr(
    borough_summary_rev['avg_council_tax']
)

print("!! Summary for Figure 01: Revenue vs Council Tax by Borough !!\n")

# Overall averages
print(f"Overall average annual revenue per listing: £{overall_rev_mean:,.0f}")
print(f"Overall average council tax (Band D):      £{overall_tax_mean:,.0f}\n")

# Top 3 revenue boroughs
print("Top 3 boroughs by average annual Airbnb revenue:")
for _, row in top3.iterrows():
    print(
        f"  - {row['borough_name'].title()}: "
        f"£{row['avg_airbnb_revenue']:,.0f} avg revenue, "
        f"£{row['avg_council_tax']:,.0f} avg council tax, "
        f"{int(row['n_listings'])} listings"
    )

# Bottom 3 revenue boroughs
print("\nBottom 3 boroughs by average annual Airbnb revenue:")
for _, row in bottom3.iterrows():
    print(
        f"  - {row['borough_name'].title()}: "
        f"£{row['avg_airbnb_revenue']:,.0f} avg revenue, "
        f"£{row['avg_council_tax']:,.0f} avg council tax, "
        f"{int(row['n_listings'])} listings"
    )

# The relationship between revenue and tax
print("\nCorrelation between average revenue and average council tax:")
print(
    f"  Pearson r ≈ {corr_rev_tax:.2f}  "
    "(+1 = strong positive, 0 = no linear relationship, -1 = strong negative)"
)
```

&nbsp;

**Summary Figure 3.1**   

- Average Airbnb revenue is much higher than council tax (≈ £28k vs ≈ £1.9k). The map shows an apparent mismatch: the highest-earning boroughs (Westminster, Kensington & Chelsea, City of London) pay some of the lowest council taxes, while outer boroughs with weak Airbnb income face higher council tax and very few listings. In short, Airbnb profitability is concentrated in the centre, and council tax has almost no bite where the profits are highest.  



```{python}
#| output: false 
# Step 07 - Council Tax Increase Scenario for Professional Hosts
# In this step, we:
#   1) Compute current net profit (revenue - council tax).
#   2) Apply a +140% council tax scenario.
#   3) Measure how much profit each listing loses in % terms.
#   4) Flag listings that lose more than a chosen threshold (e.g. 10%).

# Filter only professional full-time entire homes.
# (All subsequent calculations are done only on this group.)
listings_prof = listings_gdf[listings_gdf['host_is_pro']].copy()
print("Number of professional full-time listings:", listings_prof.shape)

# Make sure the key numeric columns are actually numeric.
# This protects us from strings, blanks, etc.
listings_prof['annual_revenue'] = pd.to_numeric(
    listings_prof['estimated_revenue_l365d'], errors='coerce'
)
listings_prof['council_tax'] = pd.to_numeric(
    listings_prof['council_tax'], errors='coerce'
)

# Drop rows that are missing revenue or council tax and print the outcome
before_na = len(listings_prof)
listings_prof = listings_prof.dropna(subset=['annual_revenue', 'council_tax']).copy()
print("  - Dropped due to missing annual_revenue/council_tax:",
      before_na - len(listings_prof),
      "| remaining:", len(listings_prof))

# ------------------------------------------------------------
# # Current vs increased council tax
# ------------------------------------------------------------

# Current net profit for each listing:
# net_profit_current = annual_revenue - current council tax.
listings_prof['net_profit_current'] = (
    listings_prof['annual_revenue'] - listings_prof['council_tax'])

# Drop listings that are already not profitable BEFORE the tax rise.
# These are not really 'damaged' by the new policy because they were
# not making money to begin with.
before_profit = len(listings_prof)
listings_prof = listings_prof[listings_prof['net_profit_current'] > 0].copy()
print("  - Dropped due to non-positive profit:",
      before_profit - len(listings_prof),
      "| remaining:", len(listings_prof))

# Apply the policy: increase council tax by 140%.
# (Change tax_increase_factor if you want to test another scenario.)
tax_increase_factor = 1.4   # + 40% council tax
listings_prof['council_tax_new'] = listings_prof['council_tax'] * tax_increase_factor

# Net profit AFTER the tax increase:
listings_prof['net_profit_new'] = (
    listings_prof['annual_revenue'] - listings_prof['council_tax_new'])

# Absolute loss in profit in £:
listings_prof['profit_loss_abs'] = (
    listings_prof['net_profit_current'] - listings_prof['net_profit_new'])

# Percentage loss in profit relative to original profit:
listings_prof['profit_loss_pct'] = (
    listings_prof['profit_loss_abs'] / listings_prof['net_profit_current'])

# Total number of listings in our profit model (after all filters above)
n_total_prof = len(listings_prof)

# ------------------------------------------------------------
# Threshold-based loss flag (easily changeable)
# ------------------------------------------------------------

# Choose the policy threshold:
# loss_threshold = 0.1  > loses more than 20% of its original profit.
loss_threshold = 0.1 

# Flag listings that lose more than this % of their original profit.
listings_prof['loss_over_threshold'] = (
    listings_prof['profit_loss_pct'] > loss_threshold
)

# Count how many professional listings are heavily affected.
n_affected = listings_prof['loss_over_threshold'].sum()

# Share of professional listings that lose more than the threshold.
pct_affected = (n_affected / n_total_prof * 100) if n_total_prof > 0 else np.nan

print(
    f"Listings losing more than {loss_threshold:.0%} of profit: "
    f"{n_affected} out of {n_total_prof} "
    f"({pct_affected:.1f}% of professional full-time listings)"
)

# Easy subset for mapping/plotting later on:
# listings_prof_affected contains only the 'heavily hit' listings.
listings_prof_affected = listings_prof[listings_prof['loss_over_threshold']].copy()

# Quick peek at the first few records used in the profit model.
display(
    listings_prof[
        ['borough_name', 'annual_revenue', 'council_tax', 'council_tax_new',
         'net_profit_current', 'net_profit_new',
         'profit_loss_pct', 'loss_over_threshold']
    ].head()
)

# ---- Output DataFrames ----
# listings_prof:
#   Professional full-time entire-home listings that:
#     - have valid annual_revenue and council_tax

# listings_prof_affected:
#   Subset of listings_prof where loss_over_threshold == True.
#   These are the “hit” professional listings that lose
#   more than 40% of their original profit under the new council tax.
```

```{python}
#| output: false 
total_revenue = listings_prof['annual_revenue'].sum()
```

```{python}
#| output: false 
# Step 09 - Borough share of listings losing >10% of profit
# Goal:
#   1) For each borough, count how many professional full-time listings
#      lose more than 10% of their original profit.
#   2) Express this as a percentage of all professional listings
#      in that borough.
#   3) This shows where hosts are *most likely* to be heavily hit
#      by the 40% council tax increase.

# Create a boolean flag for "heavily affected" listings
listings_prof['over_thresh'] = listings_prof['profit_loss_pct'] > loss_threshold

# Borough-level summary of exposure to the policy
borough_impact = (
    listings_prof
    .groupby('borough_name', as_index=False)
    .agg(
        total_listings=('id', 'count'),      # all professional full-time listings
        affected=('over_thresh', 'sum')          # how many lose >10% profit
    )
)

# Convert to percentage of listings in each borough
borough_impact['affected_share_pct'] = (
    borough_impact['affected'] / borough_impact['total_listings'] * 100
).round(1)

# Sort from most exposed borough to least
borough_impact = (
    borough_impact
    .sort_values('affected_share_pct', ascending=False)
    .reset_index(drop=True)
)

display(borough_impact)

# ---- Output table ----
# borough_impact = for each borough:
#   - total_listings: number of professional full-time listings
#   - affected: number of those losing >10% of profit
#   - affected_share_pct: % of listings in that borough that are heavily hit
```

{{< pagebreak >}}

## **Figure 3.2** | Linear Relationship Between Revenue and Profit Loss

```{python}
#| echo: false
# Step 09 - Figure 02: Linear Relationship Between Revenue and Profit Loss
# In this step we:
#   1) Trim away extreme revenue outliers (top 1%) so the plot is readable.
#   2) Look only at listings with a sensible profit-loss value (0–100%).
#   3) Scatter-plot annual revenue vs % profit lost.
#   4) Fit a simple straight line (least-squares) just to see if there is
#      any clear linear pattern between revenue level and damage from the tax.

# Trim extremes so a few huge listings don't dominate the axes
rev_q99 = listings_prof['annual_revenue'].quantile(0.99)

# Keep only:
#   - Revenue below or equal to the 99th percentile
#   - Profit loss between 0 and 1 (0–100%)
mask = (
    (listings_prof['annual_revenue'] <= rev_q99) &
    (listings_prof['profit_loss_pct'] >= 0) &
    (listings_prof['profit_loss_pct'] <= 1)   # 0–100% loss
)

# Extract the x (revenue) and y (loss proportion) for the filtered sample
x = listings_prof.loc[mask, 'annual_revenue'].values
y = listings_prof.loc[mask, 'profit_loss_pct'].values

num_used = mask.sum()
num_total = len(listings_prof)
regression_text = "Not enough data for regression." # Default text

# Safety check: only fit a line if we actually have data left
if len(x) > 1:
    # Fit a simple linear regression
    intercept, slope = np.polyfit(x, y, 1)

    # Plot the scatter and the fitted line
    fig, ax = plt.subplots(figsize=(7, 4))
    ax.scatter(x, y, s=1, alpha=0.3)
    x_line = np.linspace(x.min(), x.max(), 100)
    ax.plot(x_line, intercept + slope * x_line, linewidth=2)
    ax.set_xlabel('Annual revenue (£)')
    ax.set_ylabel('Profit loss proportion')
    ax.set_title(
        'Profit Loss vs Annual Revenue\n'
        '(99th percentile trimmed, simple fitted line)'
    )
    ax.set_ylim(0, max(y.max() * 1.1, 0.5))
    plt.tight_layout()
    # No plt.show() needed, Quarto will capture the figure
```

&nbsp;

&nbsp;

::: {.callout-important icon="false"}
## INTELLIGENCE ANALYSIS: REVENUE MODEL

**DATA INTERCEPTION**: This regression is derived from a filtered sample of **`{python} f"{num_used:,}" if 'num_used' in locals() else "N/A"`** listings.

**DERIVED FORMULA**:

```{python}
#| output: asis
#| echo: false

if 'intercept' in locals() and 'slope' in locals():
  print("$$")
  part1 = r"\text{profit\_loss\_pct} \approx"
  part2 = f"{intercept:.4f} + {slope:.8f}"
  part3 = r"\times \text{annual\_revenue}"
  print(f"{part1} {part2} {part3}")
  print("$$")
else:
  print("*(Insufficient data to derive formula)*")
```

::: 

&nbsp;

&nbsp;

**Summary Figure 3.2**   

- Low-earning listings get hit with the opposition proposal. And high-earning listings aren't affected. When revenue is small, a 40% tax increase can wipe out a big chunk of profit — sometimes almost all of it. But once revenue rises, council tax becomes tiny in comparison, so the profit loss drops close to zero.

{{< pagebreak >}}


## **Figure 3.3** | Map of Average Profit Loss per Borough

&nbsp;

&nbsp;

```{python}
#| echo: false
#| output: asis
#| freeze: false

# ------------------------------------------------------------
# -- Phase 1: Data Aggregation & Preparation
# ------------------------------------------------------------

# Clip 'profit_loss_pct' to a 0-1 range to prevent outliers from skewing the mean.
listings_prof['profit_loss_pct_clipped'] = (
    listings_prof['profit_loss_pct'].clip(lower=0, upper=1)
)

# Aggregate listing-level data to the borough level.
# For each borough, we calculate the number of listings and the average/median profit loss.
borough_loss_table = (
    listings_prof
    .groupby('borough_name', as_index=False)
    .agg(
        n_listings=('id', 'count'),                       
        avg_loss_pct=('profit_loss_pct_clipped', 'mean'),  
        median_loss_pct=('profit_loss_pct_clipped', 'median')
    )
)

# Convert proportions (0.097) to percentages (9.7) for readability.
borough_loss_table['avg_loss_pct'] = (borough_loss_table['avg_loss_pct'] * 100).round(1)
borough_loss_table['median_loss_pct'] = (borough_loss_table['median_loss_pct'] * 100).round(1)

# Rename columns for the final table presentation.
# Note: The '%' symbol is escaped as '\%' to be correctly rendered in LaTeX.
borough_loss_table = (
    borough_loss_table
    .rename(columns={
        'borough_name': 'Borough',
        'n_listings': 'Count (Pro)',
        'avg_loss_pct': 'Avg Loss (\%)',
        'median_loss_pct': 'Med Loss (\%)'
    })
    .sort_values('Avg Loss (\%)', ascending=False)
    .reset_index(drop=True)
)

# ------------------------------------------------------------
# -- Phase 2: LaTeX Table Generation
# ------------------------------------------------------------

# Generate the raw LaTeX code from the pandas DataFrame.
# 'escape=False' is crucial because we manually escaped the '%' in the column names.
latex_code = borough_loss_table.to_latex(
    index=False, 
    column_format='lrrr',
    float_format="%.1f",
    escape=False
)

# Print the raw LaTeX code. Quarto will render this into a formatted table
# because of the '#| output: asis' option at the top of this chunk.

# Start the 'table' environment for proper floating and captioning.
print(r"\begin{table}[ht]")

# Center the table and title.
print(r"\centering")

# Add a manual title above the table, with vertical spacing.
print(r"\textbf{Average Profit Loss per Borough}\\[1.5em]") 

# Use a smaller font size for the table content.
print(r"\small")
print(latex_code)

# Add a footnote below the table to explain the column names.
print(r"\vspace{1ex}")
print(r"\begin{minipage}{0.85\textwidth}")
print(r"\raggedright")
print(r"\footnotesize")
print(r"\textit{Note:} \\")
print(r"\textbf{Count (Pro)}: Number of professional full-time listings; \\")
print(r"\textbf{Avg/Med Loss (\%)}: Average and Median percentage of profit lost under the new tax policy.")
print(r"\end{minipage}")

# Close the 'table' environment.
print(r"\end{table}")
```


```{python}
#| echo: false
# Join borough polygons to the borough loss table for mapping
borough_loss_map = borough_shp.merge(
    borough_loss_table,
    left_on='borough_name',
    right_on='Borough',
    how='left'
)

# Create a clean column name to plot
borough_loss_map['avg_loss_pct_percent'] = borough_loss_map['Avg Loss (\%)']

gdf = borough_loss_map.copy() 

fig, ax = plt.subplots(figsize=(8, 8)) 

vmin = gdf['avg_loss_pct_percent'].min()
vmax = gdf['avg_loss_pct_percent'].max()

# 2. 
gdf.plot(
    column='avg_loss_pct_percent',
    cmap='Reds',      
    linewidth=0.5,
    edgecolor='white',  
    legend=True,
    vmin=vmin,
    vmax=vmax,
    legend_kwds={
        'orientation': 'horizontal', 
        'shrink': 0.7,               
        'pad': 0.01,                 
        'label': 'Average Profit Loss (\\%)', 
    },
    ax=ax
)

ax.set_axis_off()
ax.set_title(
    "Average Profit Loss per Borough (\\%)\n"
    "(Professional full-time, +40% council tax)",
    fontsize=12
)

# Add % labels at borough centroids
centroids = gdf.geometry.centroid

for (_, row), point in zip(gdf.iterrows(), centroids):
    if pd.isna(row['avg_loss_pct_percent']):
        continue
    ax.text(
        point.x,
        point.y,
        f"{row['avg_loss_pct_percent']:.0f}%",  # round to whole % for readability
        ha='center',
        va='center',
        fontsize=6,
        color='black'
    )

plt.tight_layout()
# plt.show()

# ---- Output Figure ----
# Figure 03: Choropleth map where each borough is shaded by its average
```


**Summary Figure 3.3**   

- We can see that the average profit loss per borough ranges from abou **1–5% in central boroughs** to 10% in some outer boroughs, meaning outer borough hosts are more affected by the opposition's suggestion.

```{python}
#| output: false 
# Step 11b - Text summary for borough exposure to >10% profit loss
# Context:
# Where does the 40% council tax increase bite hardest in relative terms?

print("!!Summary for Step 09: Borough exposure to >10% profit loss!!\n")

# Overall numbers across all boroughs:
#   How many professional listings lose >40% of profit in total,
#   and what % of all professional listings that represent.
overall_total = borough_impact['total_listings'].sum()
overall_affected = borough_impact['affected'].sum()
overall_share = overall_affected / overall_total * 100 if overall_total > 0 else float("nan")

print(
    f"Across all boroughs, {overall_affected:,} out of {overall_total:,} "
    f"professional full-time listings lose more than 40% of their original profit."
    f"({overall_share:.1f}% overall).\n"
)

# Top 5 boroughs by share of listings losing >40% of profit
# (not by raw counts, but by % of local hosts heavily hit).
top5 = borough_impact.head(5)

print("Top 5 boroughs by share of listings losing >10% of profit:")
for _, row in top5.iterrows():
    print(
        f"  - {row['borough_name'].title()}: "
        f"{int(row['affected'])} out of {int(row['total_listings'])} listings "
        f"({row['affected_share_pct']:.1f}% heavily affected)"
    )

# Bottom 3 boroughs (least exposed to the policy in % terms).
bottom3 = borough_impact.tail(3)

print("\nBottom 3 boroughs by share of listings losing >10% of profit:")
for _, row in bottom3.iterrows():
    print(
        f"  - {row['borough_name'].title()}: "
        f"{int(row['affected'])} out of {int(row['total_listings'])} listings "
        f"({row['affected_share_pct']:.1f}% heavily affected)"
    )

print(
    "\nThese figures show where the 40% council tax increase *affects hardest* in\n"
    "relative terms (high affected_share_pct), even if some of those boroughs\n"
    "do not have the largest absolute number of listings."
)
```

&nbsp;

&nbsp;

**Short General Conclusion on discussion 3:**  

- Only a small share of professional landlords are meaningfully affected by a 40% council tax increase:**272 out of 8,849** listings (about 3%) lose more than 10% of their profit. The impact is concentrated in outer London and lower-revenue boroughs, while high-revenue central boroughs show almost no effect. This suggests that a flat council tax rise is not a strong lever for changing professional Airbnb behaviour; more targeted measures based on revenue or the number of listings would be needed to shift the sector in any meaningful way.

{{< pagebreak >}}

# **Policy Analysis: Regulating Short-Term Lets for Housing Equity**

1. **The Context: The Scale of the Problem**  
Our analysis shows that Airbnb contributes to London’s housing shortage. London adds about **35,000 homes per year** (@greaterlondonauthority), **while Airbnb introduces 5,000–7,200 new listings annually (Q1)**. As a result, **14–20%** of new housing is absorbed by the short-term rental market.  

&nbsp;

2. **Critique of the Opposition’s Proposal**  
The opposition proposes raising council tax for Airbnb hosts and "professional landlords." While this addresses the issue, our analysis data shows this approach is **blunt and inconsistent**.  
- "Professional landlords" as such, with multiple listings, especially in central boroughs such as Westminster and Kensington & Chelsea, achieve higher profits. Even with a suggested **40% council tax increase (Q3)**, most high-revenue listings remain profitable.  
- This “minimal” tax disproportionately impacts lower-revenue, casual hosts in outer boroughs who cannot absorb the cost, while failing to deter well-capitalised operators who most affect housing supply.  

&nbsp;

3. **The Better Alternative: Comprehensive Regulation**  
Rather than relying solely on council tax, the Mayor should support the opposition’s intent while adopting a different approach. Effective policy that can address the market’s underlying mechanisms:  
- **Registration and Enforcement**: Require strict host registration and impose penalties on non-compliant platforms.
Primary Residence Rules: Adopt international standards, such as **New York’s Local Law 18**  (@nycose) and **Amsterdam’s 30-night cap** (@amsterdam), that require hosts to be primary residents.
- **Smart Taxation**: Replace flat council tax increases with a combined **revenue-based and volume-based tax system** that targets high-yield portfolios (Q2).
- **Supply Protection**: Ban short-term letting of council housing and newly built units.

&nbsp;

4. **Reframing the Narrative: Social Mobility and Opportunity**  
Regulating Airbnb should be seen not just as a tax issue, but as a positive step for **social mobility and housing opportunity**. London Councils (2025) (-@londoncouncils) says that 24% of renters feel secure about continuing to live in the city, and 1 in 4 of them thinks they may need to leave the city to find more affordable rent in the next 12 months. In addition, Councillor Claire Holland claims:  

> *"London faces the most severe homelessness emergency in the country. Driven by the worsening shortage of affordable housing, far too many Londoners are struggling with their housing costs and at risk of becoming homeless".*


{{< pagebreak >}}

&nbsp;
&nbsp;
&nbsp;

\begin{center}
\Large\bfseries
Pros and Cons of the Council Tax Proposal
\end{center}

&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

\vspace{0.5cm}
\begin{center}
\renewcommand{\arraystretch}{1.4}
\setlength{\arrayrulewidth}{1.5pt}

\definecolor{IntelRed}{HTML}{8B0000}
\definecolor{IntelBlue}{HTML}{000080}

\begin{tabular}{|p{0.95\textwidth}|}
    \hline
    \textbf{\texttt{>> IMPACT ASSESSMENT MATRIX (CLASSIFIED)}} \\
    \hline
    
    % --- Section 1: Mayor ---
    \vspace{0.1cm}
    \textbf{\large 1. TARGET: MAYORAL OFFICE} \\
    \vspace{0.1cm}
    
    \textbf{\textcolor{IntelBlue}{[+] STRATEGIC GAINS}}
    \begin{itemize}
        \item \textbf{Narrative Control}: Mayor can claim victory in "Making Airbnb pay more".
        \item \textbf{Resource Acquisition}: Generates new revenue streams for enforcement.
    \end{itemize}
    
    \textbf{\textcolor{IntelRed}{[-] TACTICAL RISKS}}
    \begin{itemize}
        \item \textbf{Symbolic Policy}: Risk of being perceived as performative rather than practical.
        \item \textbf{Asymmetric Impact}: Penalizes small hosts in outer boroughs; wealthy operators in Westminster remain unaffected.
    \end{itemize}
    \\ \hline

    % --- Section 2: Residents ---
    \vspace{0.1cm}
    \textbf{\large 2. TARGET: CIVILIAN POPULATION} \\
    \vspace{0.1cm}
    
    \textbf{\textcolor{IntelBlue}{[+] STRATEGIC GAINS}}
    \begin{itemize}
        \item \textbf{Supply Recovery}: Marginal listings may revert to long-term housing as profits thin.
    \end{itemize}
    
    \textbf{\textcolor{IntelRed}{[-] TACTICAL RISKS}}
    \begin{itemize}
        \item \textbf{Profit Gap Persists}: Short-term rentals remain far more profitable; most landlords will absorb the tax.
    \end{itemize}
    \\ \hline
    
    % --- Section 3: City ---
    \vspace{0.1cm}
    \textbf{\large 3. TARGET: CITY INFRASTRUCTURE} \\
    \vspace{0.1cm}
    
    \textbf{\textcolor{IntelBlue}{[+] STRATEGIC GAINS}}
    \begin{itemize}
        \item \textbf{Demand Shift}: Reduces pressure on residential zones; pushes tourists back to hotels.
    \end{itemize}
    
    \textbf{\textcolor{IntelRed}{[-] TACTICAL RISKS}}
    \begin{itemize}
        \item \textbf{Misaligned Targeting}: Policy targets tax bands rather than the root drivers (Revenue \& Portfolio Size).
    \end{itemize}
    \vspace{0.1cm}
    \\ \hline
    
\end{tabular}
\end{center}



{{< pagebreak >}}


Drawing on Dayne Lee’s study of the Los Angeles housing market (@DayneLee), this narrative shift could help see Airbnb regulation as an opportunity to prioritize residential stability in high-demand areas. Following this, policy measures can be justified by:
- Protecting Communities: Preventing long-term rentals from becoming short-term lets helps safeguard against eviction and displacement.
- Funding Affordability: Revenue from high-volume landlords should be redistributed to less privileged boroughs to support social housing programs.
- Stopping Gentrification: Reducing the profitability of short-term lets slows neighbourhood change that displaces lower-income households from quality amenities.

## Implications for the Mayor
In the context of the election, the opposition’s proposal could be seen as merely a response to individual misconduct, allowing the Mayor to shift the narrative to a broader, more powerful regulation in the collective interest. The strategy presented combines registration, regulation, and redistribution to less privileged areas of the city. This would show responsiveness from the current administration while addressing a structural issue about housing instability.  

&nbsp;

This is key to the campaign's communication: it acknowledges public concern and leverages it. Doing this enables the Mayor to shift the debate from political blame to policy effectiveness, using data to justify a proportionate and socially equitable response.







{{< pagebreak >}}


# References

